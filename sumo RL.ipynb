{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f4fc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from CustomGymEnvSetup import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e79a5b",
   "metadata": {},
   "source": [
    "## Test of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcae21e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('instigo-goma-rl-v0',\n",
    "               #sumoconfig_file=\"E:/Instigo-TS-CTRL/network/osm.sumocfg\",\n",
    "               sumoconfig_file=\"E:/Instigo-TS-CTRL/network_trainning/single-intersection.sumocfg\",\n",
    "                use_gui=True,\n",
    "                #num_seconds=41000,\n",
    "               num_seconds=100000, #for the single_intersection.sumocfg\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c00186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.07925325818950335, 0.07925325818950335]\n",
      "++++ Last Density :  0.0\n",
      "++++ NEW Density :  0.07925325818950335\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.07925325818950335\n",
      "\n",
      "++++ Phases density :  [0.09246213455442057, 0.06604438182458612]\n",
      "++++ Last Density :  0.07925325818950335\n",
      "++++ NEW Density :  0.07925325818950335\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.1056710109193378, 0.039626629094751675]\n",
      "++++ Last Density :  0.07925325818950335\n",
      "++++ NEW Density :  0.07264882000704473\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.006604438182458622\n",
      "\n",
      "++++ Phases density :  [0.13208876364917227, 0.039626629094751675]\n",
      "++++ Last Density :  0.07264882000704473\n",
      "++++ NEW Density :  0.08585769637196197\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917244\n",
      "\n",
      "++++ Phases density :  [0.17171539274392392, 0.02641775272983445]\n",
      "++++ Last Density :  0.08585769637196197\n",
      "++++ NEW Density :  0.09906657273687919\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917216\n",
      "\n",
      "++++ Phases density :  [0.1585065163790067, 0.07925325818950335]\n",
      "++++ Last Density :  0.09906657273687919\n",
      "++++ NEW Density :  0.11887988728425503\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.019813314547375838\n",
      "\n",
      "++++ Phases density :  [0.18492426910884113, 0.06604438182458612]\n",
      "++++ Last Density :  0.11887988728425503\n",
      "++++ NEW Density :  0.12548432546671362\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.19813314547375838, 0.039626629094751675]\n",
      "++++ Last Density :  0.12548432546671362\n",
      "++++ NEW Density :  0.11887988728425503\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.2113420218386756, 0.039626629094751675]\n",
      "++++ Last Density :  0.11887988728425503\n",
      "++++ NEW Density :  0.12548432546671362\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.2509686509334273, 0.039626629094751675]\n",
      "++++ Last Density :  0.12548432546671362\n",
      "++++ NEW Density :  0.14529764001408949\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.019813314547375865\n",
      "\n",
      "++++ Phases density :  [0.23775977456851005, 0.09246213455442057]\n",
      "++++ Last Density :  0.14529764001408949\n",
      "++++ NEW Density :  0.1651109545614653\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.01981331454737581\n",
      "\n",
      "++++ Phases density :  [0.2509686509334273, 0.09246213455442057]\n",
      "++++ Last Density :  0.1651109545614653\n",
      "++++ NEW Density :  0.17171539274392394\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.0066044381824586496\n",
      "\n",
      "++++ Phases density :  [0.25096865093342724, 0.07925325818950335]\n",
      "++++ Last Density :  0.17171539274392394\n",
      "++++ NEW Density :  0.1651109545614653\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0066044381824586496\n",
      "\n",
      "++++ Phases density :  [0.23775977456851005, 0.07925325818950335]\n",
      "++++ Last Density :  0.1651109545614653\n",
      "++++ NEW Density :  0.1585065163790067\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.30380415639309616, 0.06604438182458612]\n",
      "++++ Last Density :  0.1585065163790067\n",
      "++++ NEW Density :  0.18492426910884113\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.02641775272983443\n",
      "\n",
      "++++ Phases density :  [0.29059528002817897, 0.06604438182458612]\n",
      "++++ Last Density :  0.18492426910884113\n",
      "++++ NEW Density :  0.17831983092638254\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.34343078548784783, 0.11887988728425503]\n",
      "++++ Last Density :  0.17831983092638254\n",
      "++++ NEW Density :  0.23115533638605143\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.05283550545966889\n",
      "\n",
      "++++ Phases density :  [0.36984853821768227, 0.11887988728425503]\n",
      "++++ Last Density :  0.23115533638605143\n",
      "++++ NEW Density :  0.24436421275096865\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917216\n",
      "\n",
      "++++ Phases density :  [0.39626629094751675, 0.11887988728425503]\n",
      "++++ Last Density :  0.24436421275096865\n",
      "++++ NEW Density :  0.2575730891158859\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917244\n",
      "\n",
      "++++ Phases density :  [0.4491017964071856, 0.11887988728425503]\n",
      "++++ Last Density :  0.2575730891158859\n",
      "++++ NEW Density :  0.2839908418457203\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.02641775272983443\n",
      "\n",
      "++++ Phases density :  [0.5019373018668545, 0.11887988728425503]\n",
      "++++ Last Density :  0.2839908418457203\n",
      "++++ NEW Density :  0.31040859457555475\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.02641775272983443\n",
      "\n",
      "++++ Phases density :  [0.5151461782317718, 0.11887988728425503]\n",
      "++++ Last Density :  0.31040859457555475\n",
      "++++ NEW Density :  0.3170130327580134\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.0066044381824586496\n",
      "\n",
      "++++ Phases density :  [0.5811905600563578, 0.19813314547375838]\n",
      "++++ Last Density :  0.3170130327580134\n",
      "++++ NEW Density :  0.3896618527650581\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.0726488200070447\n",
      "\n",
      "++++ Phases density :  [0.5943994364212751, 0.19813314547375838]\n",
      "++++ Last Density :  0.3896618527650581\n",
      "++++ NEW Density :  0.39626629094751675\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.0066044381824586496\n",
      "\n",
      "++++ Phases density :  [0.647234941880944, 0.19813314547375838]\n",
      "++++ Last Density :  0.39626629094751675\n",
      "++++ NEW Density :  0.4226840436773512\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.02641775272983443\n",
      "\n",
      "++++ Phases density :  [0.647234941880944, 0.19813314547375838]\n",
      "++++ Last Density :  0.4226840436773512\n",
      "++++ NEW Density :  0.4226840436773512\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.6736526946107785, 0.19813314547375838]\n",
      "++++ Last Density :  0.4226840436773512\n",
      "++++ NEW Density :  0.43589292004226843\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917244\n",
      "\n",
      "++++ Phases density :  [0.6736526946107785, 0.19813314547375838]\n",
      "++++ Last Density :  0.43589292004226843\n",
      "++++ NEW Density :  0.43589292004226843\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.700070447340613, 0.25096865093342724]\n",
      "++++ Last Density :  0.43589292004226843\n",
      "++++ NEW Density :  0.4755195491370201\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.039626629094751675\n",
      "\n",
      "++++ Phases density :  [0.700070447340613, 0.25096865093342724]\n",
      "++++ Last Density :  0.4755195491370201\n",
      "++++ NEW Density :  0.4755195491370201\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.7264882000704473, 0.25096865093342724]\n",
      "++++ Last Density :  0.4755195491370201\n",
      "++++ NEW Density :  0.4887284255019373\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.7264882000704473, 0.25096865093342724]\n",
      "++++ Last Density :  0.4887284255019373\n",
      "++++ NEW Density :  0.4887284255019373\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.7529059528002818, 0.25096865093342724]\n",
      "++++ Last Density :  0.4887284255019373\n",
      "++++ NEW Density :  0.5019373018668545\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.7529059528002818, 0.25096865093342724]\n",
      "++++ Last Density :  0.5019373018668545\n",
      "++++ NEW Density :  0.5019373018668545\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.7793237055301162, 0.3302219091229306]\n",
      "++++ Last Density :  0.5019373018668545\n",
      "++++ NEW Density :  0.5547728073265235\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.052835505459668974\n",
      "\n",
      "++++ Phases density :  [0.7793237055301162, 0.3302219091229306]\n",
      "++++ Last Density :  0.5547728073265235\n",
      "++++ NEW Density :  0.5547728073265235\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.8057414582599508, 0.3302219091229306]\n",
      "++++ Last Density :  0.5547728073265235\n",
      "++++ NEW Density :  0.5679816836914406\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.8057414582599508, 0.3302219091229306]\n",
      "++++ Last Density :  0.5679816836914406\n",
      "++++ NEW Density :  0.5679816836914406\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.8321592109897852, 0.3302219091229306]\n",
      "++++ Last Density :  0.5679816836914406\n",
      "++++ NEW Density :  0.5811905600563578\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.8585769637196196, 0.38305741458259956]\n",
      "++++ Last Density :  0.5811905600563578\n",
      "++++ NEW Density :  0.6208171891511096\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.039626629094751786\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.8585769637196196, 0.38305741458259956]\n",
      "++++ Last Density :  0.6208171891511096\n",
      "++++ NEW Density :  0.6208171891511096\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.8849947164494542, 0.38305741458259956]\n",
      "++++ Last Density :  0.6208171891511096\n",
      "++++ NEW Density :  0.6340260655160268\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.8849947164494542, 0.38305741458259956]\n",
      "++++ Last Density :  0.6340260655160268\n",
      "++++ NEW Density :  0.6340260655160268\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9114124691792885, 0.38305741458259956]\n",
      "++++ Last Density :  0.6340260655160268\n",
      "++++ NEW Density :  0.647234941880944\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9114124691792885, 0.38305741458259956]\n",
      "++++ Last Density :  0.647234941880944\n",
      "++++ NEW Density :  0.647234941880944\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9246213455442057, 0.46231067277210286]\n",
      "++++ Last Density :  0.647234941880944\n",
      "++++ NEW Density :  0.6934660091581543\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.04623106727721027\n",
      "\n",
      "++++ Phases density :  [0.9246213455442057, 0.46231067277210286]\n",
      "++++ Last Density :  0.6934660091581543\n",
      "++++ NEW Density :  0.6934660091581543\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.937830221909123, 0.46231067277210286]\n",
      "++++ Last Density :  0.6934660091581543\n",
      "++++ NEW Density :  0.700070447340613\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.006604438182458705\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.46231067277210286]\n",
      "++++ Last Density :  0.700070447340613\n",
      "++++ NEW Density :  0.7066748855230716\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.006604438182458594\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.46231067277210286]\n",
      "++++ Last Density :  0.7066748855230716\n",
      "++++ NEW Density :  0.7066748855230716\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7066748855230716\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.026417752729834376\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5151461782317718]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7330926382529059\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7330926382529059\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.039626629094751786\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.5943994364212751]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7727192673476577\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6208171891511096]\n",
      "++++ Last Density :  0.7727192673476577\n",
      "++++ NEW Density :  0.7859281437125749\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6208171891511096]\n",
      "++++ Last Density :  0.7859281437125749\n",
      "++++ NEW Density :  0.7859281437125749\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6208171891511096]\n",
      "++++ Last Density :  0.7859281437125749\n",
      "++++ NEW Density :  0.7859281437125749\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6208171891511096]\n",
      "++++ Last Density :  0.7859281437125749\n",
      "++++ NEW Density :  0.7859281437125749\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6208171891511096]\n",
      "++++ Last Density :  0.7859281437125749\n",
      "++++ NEW Density :  0.7859281437125749\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7859281437125749\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.647234941880944]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.7991370200774921\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.7991370200774921\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917299\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.6736526946107786]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8123458964424094\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8123458964424094\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.700070447340613]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8255547728073266\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8255547728073266\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7264882000704473]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8387636491722438\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8387636491722438\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  -0.013208876364917299\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7529059528002818]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8519725255371611\n",
      "+++++ INFO ++++ \n",
      " Action :  1\n",
      " Reward :  0.0\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7793237055301163]\n",
      "++++ Last Density :  0.8519725255371611\n",
      "++++ NEW Density :  0.8651814019020783\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  -0.013208876364917188\n",
      "\n",
      "++++ Phases density :  [0.9510390982740402, 0.7793237055301163]\n",
      "++++ Last Density :  0.8651814019020783\n",
      "++++ NEW Density :  0.8651814019020783\n",
      "+++++ INFO ++++ \n",
      " Action :  0\n",
      " Reward :  0.0\n",
      "\n"
     ]
    },
    {
     "ename": "FatalTraCIError",
     "evalue": "Connection closed by SUMO.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()  \u001b[38;5;66;03m# agent policy that uses the observation and info\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#obs, rewards, terminated, truncated, info = vec_env.step(action)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+++++ INFO ++++ \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Action : \u001b[39m\u001b[38;5;124m\"\u001b[39m, action)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:228\u001b[0m, in \u001b[0;36mSumoEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# if self.fixed_ts:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m#     self.traffic_signal.sumo.trafficlight.setRedYellowGreenState(\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m#             self.ts_id, self.traffic_signal.all_phases[self.fixed_ts_phase_id].state\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_action(action)\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_observation()\n\u001b[0;32m    231\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_reward()\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:243\u001b[0m, in \u001b[0;36mSumoEnvironment._run_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m time_to_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_to_act:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sumo_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_signal\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_signal\u001b[38;5;241m.\u001b[39mtime_to_act:\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:313\u001b[0m, in \u001b[0;36mSumoEnvironment._sumo_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sumo_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:368\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    367\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI change now handles step as floating point seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptionMapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    370\u001b[0m     subscriptionResults\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:137\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection closed by SUMO.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m command \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue:\n\u001b[0;32m    139\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BBB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: Connection closed by SUMO."
     ]
    }
   ],
   "source": [
    "obs, info = env.reset()\n",
    "\n",
    "for i in range(100000):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    #obs, rewards, terminated, truncated, info = vec_env.step(action)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    print(f\"+++++ INFO ++++ \")\n",
    "    print(f\" Action : \", action)\n",
    "    print(f\" Reward : \", reward)\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa798155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91f520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ef75da",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4a56eb",
   "metadata": {},
   "source": [
    "## Training with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8ede822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.dqn.dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83f47c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('instigo-goma-rl-v0',\n",
    "               sumoconfig_file=\"E:/Instigo-TS-CTRL/network/osm.sumocfg\",\n",
    "                use_gui=False,\n",
    "                num_seconds=41000,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fa6b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_Log_path = os.path.join('Training', 'DQN', 'Logs')\n",
    "DQN_path = os.path.join('Training', 'DQN_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f98c3a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = DQN(\n",
    "        env=env,\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        learning_rate=0.001,\n",
    "        learning_starts=0,\n",
    "        train_freq=1,\n",
    "        target_update_interval=500,\n",
    "        exploration_initial_eps=0.05,\n",
    "        exploration_final_eps=0.01,\n",
    "        verbose=1,\n",
    "        tensorboard_log=DQN_Log_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f418b8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\DQN\\Logs\\DQN_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SumoEnvironment.__del__ at 0x000001D8A3EB28C0>\n",
      "Traceback (most recent call last):\n",
      "  File \"E:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py\", line 329, in __del__\n",
      "  File \"E:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py\", line 319, in close\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\", line 262, in close\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 396, in close\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 231, in _sendCmd\n",
      "  File \"C:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py\", line 137, in _sendExact\n",
      "traci.exceptions.FatalTraCIError: Connection closed by SUMO.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:267\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfDQN,\n\u001b[0;32m    260\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfDQN:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:218\u001b[0m, in \u001b[0;36mSumoEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    216\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# there are no 'terminal' states in this environment\u001b[39;00m\n\u001b[0;32m    217\u001b[0m truncated \u001b[38;5;241m=\u001b[39m dones[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__all__\u001b[39m\u001b[38;5;124m\"\u001b[39m]  \u001b[38;5;66;03m# episode ends when sim_step >= max_steps\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, terminated, truncated, info\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:252\u001b[0m, in \u001b[0;36mSumoEnvironment._compute_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim_step}\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_system_info:\n\u001b[1;32m--> 252\u001b[0m     info\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_system_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mappend(info\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m info\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:297\u001b[0m, in \u001b[0;36mSumoEnvironment._get_system_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m vehicles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetIDList()\n\u001b[0;32m    296\u001b[0m speeds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetSpeed(vehicle) \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m vehicles]\n\u001b[1;32m--> 297\u001b[0m waiting_times \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetWaitingTime(vehicle) \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m vehicles]\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# In SUMO, a vehicle is considered halting if its speed is below 0.1 m/s\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_total_stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mint\u001b[39m(speed \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m speed \u001b[38;5;129;01min\u001b[39;00m speeds),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_mean_speed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vehicles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(speeds),\n\u001b[0;32m    304\u001b[0m }\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:297\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    295\u001b[0m vehicles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetIDList()\n\u001b[0;32m    296\u001b[0m speeds \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msumo\u001b[38;5;241m.\u001b[39mvehicle\u001b[38;5;241m.\u001b[39mgetSpeed(vehicle) \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m vehicles]\n\u001b[1;32m--> 297\u001b[0m waiting_times \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvehicle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetWaitingTime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvehicle\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m vehicle \u001b[38;5;129;01min\u001b[39;00m vehicles]\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# In SUMO, a vehicle is considered halting if its speed is below 0.1 m/s\u001b[39;00m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_total_stopped\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mint\u001b[39m(speed \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m speed \u001b[38;5;129;01min\u001b[39;00m speeds),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_mean_speed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(vehicles) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(speeds),\n\u001b[0;32m    304\u001b[0m }\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\_vehicle.py:554\u001b[0m, in \u001b[0;36mVehicleDomain.getWaitingTime\u001b[1;34m(self, vehID)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetWaitingTime\u001b[39m(\u001b[38;5;28mself\u001b[39m, vehID):\n\u001b[0;32m    548\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"getWaitingTime(string) -> double\u001b[39;00m\n\u001b[0;32m    549\u001b[0m \u001b[38;5;124;03m    The waiting time of a vehicle is defined as the time (in seconds) spent with a\u001b[39;00m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;124;03m    speed below 0.1m/s since the last time it was faster than 0.1m/s.\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;124;03m    (basically, the waiting time of a vehicle is reset to 0 every time it moves).\u001b[39;00m\n\u001b[0;32m    552\u001b[0m \u001b[38;5;124;03m    A vehicle that is stopping intentionally with a <stop> does not accumulate waiting time.\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getUniversal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVAR_WAITING_TIME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvehID\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:147\u001b[0m, in \u001b[0;36mDomain._getUniversal\u001b[1;34m(self, varID, objectID, format, *values)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor:\n\u001b[0;32m    146\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe domain \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated, use \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deprecatedFor))\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _parse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retValFunc, varID, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\domain.py:152\u001b[0m, in \u001b[0;36mDomain._getCmd\u001b[1;34m(self, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot connected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 152\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmdGetID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvarID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m r\u001b[38;5;241m.\u001b[39mreadLength()\n\u001b[0;32m    154\u001b[0m response, retVarID \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:131\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msending\u001b[39m\u001b[38;5;124m\"\u001b[39m, Storage(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\u001b[38;5;241m.\u001b[39mgetDebugString())\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39msend(length \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string)\n\u001b[1;32m--> 131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recvExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _DEBUG:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceiving\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mgetDebugString())\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:109\u001b[0m, in \u001b[0;36mConnection._recvExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytes\u001b[39m()\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m--> 109\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=40000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5e790a",
   "metadata": {},
   "source": [
    "### Saving the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047d9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(DQN_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2daecc1",
   "metadata": {},
   "source": [
    "### Test The DQN Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fe3dac9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `sumo-rl` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msumo-rl-v0\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m               \u001b[49m\u001b[43msumoconfig_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mE:/sumogym-TL-Ctrl/network_trainning/single-intersection-new.sumocfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43muse_gui\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnum_seconds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m41000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m              \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m obs, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      7\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\registration.py:741\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, max_episode_steps, autoreset, apply_api_compatibility, disable_env_checker, **kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[1;32m--> 741\u001b[0m     env_spec \u001b[38;5;241m=\u001b[39m \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[0;32m    745\u001b[0m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\registration.py:527\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(env_id)\u001b[0m\n\u001b[0;32m    521\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    522\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    524\u001b[0m     )\n\u001b[0;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 527\u001b[0m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mError(\n\u001b[0;32m    529\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    530\u001b[0m     )\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\registration.py:393\u001b[0m, in \u001b[0;36m_check_version_exists\u001b[1;34m(ns, name, version)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    395\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\envs\\registration.py:370\u001b[0m, in \u001b[0;36m_check_name_exists\u001b[1;34m(ns, name)\u001b[0m\n\u001b[0;32m    367\u001b[0m namespace_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m suggestion_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`?\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mNameNotFound(\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m )\n",
      "\u001b[1;31mNameNotFound\u001b[0m: Environment `sumo-rl` doesn't exist."
     ]
    }
   ],
   "source": [
    "env = gym.make('sumo-rl-v0',\n",
    "               sumoconfig_file=\"E:/sumogym-TL-Ctrl/network_trainning/single-intersection-new.sumocfg\",\n",
    "                use_gui=True,\n",
    "                num_seconds=41000,\n",
    "              )\n",
    "obs, info = env.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237cea74",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(DQN_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d072fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "\n",
    "for i in range(100000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    #obs, rewards, terminated, truncated, info = vec_env.step(action)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    \n",
    "    print(f\" Action : \", action)\n",
    "    print(f\" Reward : \", rewards)\n",
    "    print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4e6a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68924a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ae59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4180608d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9705b959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98395ff2",
   "metadata": {},
   "source": [
    "## Training with PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e083a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import gymnasium as gym\n",
    "from CustomGymEnvSetup import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "949e2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('instigo-goma-rl-v0',\n",
    "               #sumoconfig_file=\"E:/Instigo-TS-CTRL/network/osm.sumocfg\",\n",
    "               sumoconfig_file=\"E:/Instigo-TS-CTRL/network_trainning/single-intersection-new.sumocfg\",\n",
    "                use_gui=True,\n",
    "                #num_seconds=41000,\n",
    "               num_seconds=100000, #for the single_intersection.sumocfg\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dee7823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'density': array([0., 0., 0., 0.]),\n",
       "  'nb_veh': array([0, 0, 0, 0]),\n",
       "  'phase': array([1, 0])},\n",
       " {'step': 0.0,\n",
       "  'agent_total_vehicles_passed': [0],\n",
       "  'agent_total_fuel_consumption': [0.0],\n",
       "  'agent_co2_emission': [0.0],\n",
       "  'agent_accumulated_waiting_time': [0.0]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "389db703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict('density': Box(0.0, 20.0, (4,), float64), 'nb_veh': Box(0, 100, (4,), int32), 'phase': Box(0, 1, (2,), int32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e29cf2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26d9fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_Log_path = os.path.join('Training', 'PPO', 'Logs')\n",
    "PPO_path = os.path.join('Training', 'PPO_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f826f52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\PPO\\Logs\\PPO_9\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 18   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 112  |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 16          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019614976 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.676      |\n",
      "|    explained_variance   | -2.23       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0493     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    value_loss           | 0.00901     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 17          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 351         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015619863 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.0162      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0335      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00618    |\n",
      "|    value_loss           | 0.00103     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 20           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 396          |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044862335 |\n",
      "|    clip_fraction        | 0.0386       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.665       |\n",
      "|    explained_variance   | -5.45        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0277       |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00415     |\n",
      "|    value_loss           | 0.00341      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 441          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069338437 |\n",
      "|    clip_fraction        | 0.0773       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.666       |\n",
      "|    explained_variance   | -0.389       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0197      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00573     |\n",
      "|    value_loss           | 0.000552     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 496         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006630931 |\n",
      "|    clip_fraction        | 0.0474      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.698       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00181     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 0.000375    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 597         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007288379 |\n",
      "|    clip_fraction        | 0.0371      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.637      |\n",
      "|    explained_variance   | 0.415       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0113      |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00565    |\n",
      "|    value_loss           | 0.000644    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 23          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 705         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009783981 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.0105      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00368    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00785    |\n",
      "|    value_loss           | 0.000539    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 812         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007112707 |\n",
      "|    clip_fraction        | 0.0914      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.596       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00846     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0053     |\n",
      "|    value_loss           | 0.000378    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1.58e+04   |\n",
      "|    ep_rew_mean          | -0.356     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 21         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 934        |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01442492 |\n",
      "|    clip_fraction        | 0.229      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.661     |\n",
      "|    explained_variance   | 0.223      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0456    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    value_loss           | 0.000472   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 22          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1003        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011708915 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.632      |\n",
      "|    explained_variance   | 0.682       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0109     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 0.000356    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.356       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 23           |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 1042         |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067612976 |\n",
      "|    clip_fraction        | 0.0733       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.565       |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00582     |\n",
      "|    value_loss           | 0.000225     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 1082        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006728787 |\n",
      "|    clip_fraction        | 0.0526      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00958    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00278    |\n",
      "|    value_loss           | 0.000146    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 1137        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008545108 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.525      |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00268    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00551    |\n",
      "|    value_loss           | 7.86e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.356      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 25          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 1224        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012927618 |\n",
      "|    clip_fraction        | 0.0926      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.49       |\n",
      "|    explained_variance   | 0.917       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00709     |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    value_loss           | 0.000172    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.336       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 24           |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 1322         |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033346075 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.855        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0352      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00652     |\n",
      "|    value_loss           | 7.55e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.336      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 24          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 1434        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015265893 |\n",
      "|    clip_fraction        | 0.089       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00588     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00488    |\n",
      "|    value_loss           | 0.000174    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.336       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 18           |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 1957         |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058603566 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.53         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.012        |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.002       |\n",
      "|    value_loss           | 0.000285     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.336       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 15           |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 2523         |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040407213 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.563       |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00604     |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00224     |\n",
      "|    value_loss           | 0.000196     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.336       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 2932         |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0087524885 |\n",
      "|    clip_fraction        | 0.0417       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.493       |\n",
      "|    explained_variance   | 0.934        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0117       |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    value_loss           | 0.000237     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.336      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 3081        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002531589 |\n",
      "|    clip_fraction        | 0.0354      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.484      |\n",
      "|    explained_variance   | 0.939       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0129     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00202    |\n",
      "|    value_loss           | 5.86e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.58e+04    |\n",
      "|    ep_rew_mean          | -0.336      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3611        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003961308 |\n",
      "|    clip_fraction        | 0.045       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.494      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0227      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    value_loss           | 3.22e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.58e+04     |\n",
      "|    ep_rew_mean          | -0.336       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 4311         |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062357346 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0251      |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00306     |\n",
      "|    value_loss           | 0.000186     |\n",
      "------------------------------------------\n",
      " Retrying in 1 seconds\n",
      "Could not connect to TraCI server at localhost:11449 [WinError 10061] Aucune connexion n’a pu être établie car l’ordinateur cible l’a expressément refusée\n",
      " Retrying in 1 seconds\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.331      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 5138        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016813988 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.896       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.011      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 0.000127    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.331      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 6012        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010323261 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.466      |\n",
      "|    explained_variance   | 0.915       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00498    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00743    |\n",
      "|    value_loss           | 0.000186    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.331       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8            |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 6445         |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068742745 |\n",
      "|    clip_fraction        | 0.0818       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.51        |\n",
      "|    explained_variance   | 0.594        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0189       |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00337     |\n",
      "|    value_loss           | 0.000264     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.331       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 8            |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 6493         |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053269863 |\n",
      "|    clip_fraction        | 0.0805       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.506       |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0116      |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00479     |\n",
      "|    value_loss           | 0.000178     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.331      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 8           |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 6530        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008194724 |\n",
      "|    clip_fraction        | 0.0994      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.434      |\n",
      "|    explained_variance   | 0.46        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0231     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00456    |\n",
      "|    value_loss           | 0.00022     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.331       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9            |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 6568         |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048440276 |\n",
      "|    clip_fraction        | 0.0234       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.354       |\n",
      "|    explained_variance   | 0.955        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00833     |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000678    |\n",
      "|    value_loss           | 4.26e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.331       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9            |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 6635         |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019024939 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.35        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00083      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 2.68e-05     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.331      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 6757        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038938813 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.414      |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0669     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00953    |\n",
      "|    value_loss           | 0.000231    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.327       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 9            |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 6856         |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0091499165 |\n",
      "|    clip_fraction        | 0.0972       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.927        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0163      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.0103      |\n",
      "|    value_loss           | 7.6e-05      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.327      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 6990        |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014304457 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.472      |\n",
      "|    explained_variance   | 0.524       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0117     |\n",
      "|    value_loss           | 0.000559    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.327      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 9           |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 7088        |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006979352 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.488      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0161     |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00931    |\n",
      "|    value_loss           | 0.000403    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.327      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 7146        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019908164 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.467      |\n",
      "|    explained_variance   | 0.929       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0348     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | 0.00184     |\n",
      "|    value_loss           | 0.000127    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.327       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 7191         |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025613634 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 0.81         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0134      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 0.000124     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.327      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 7236        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009553277 |\n",
      "|    clip_fraction        | 0.0642      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.309      |\n",
      "|    explained_variance   | 0.936       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00284    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00381    |\n",
      "|    value_loss           | 3.07e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.327       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 7327         |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076669725 |\n",
      "|    clip_fraction        | 0.0538       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.959        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0225      |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    value_loss           | 5.01e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1.59e+04    |\n",
      "|    ep_rew_mean          | -0.334      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 10          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 7437        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010333311 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.52       |\n",
      "|    explained_variance   | 0.822       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    value_loss           | 0.000285    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1.59e+04     |\n",
      "|    ep_rew_mean          | -0.334       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 10           |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 7545         |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086515825 |\n",
      "|    clip_fraction        | 0.124        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.471       |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00497      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0104      |\n",
      "|    value_loss           | 0.000113     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x11fd87f8bb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO('MultiInputPolicy', env, verbose=1, tensorboard_log=PPO_Log_path)\n",
    "model.learn(total_timesteps=80000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfb4d2",
   "metadata": {},
   "source": [
    "### Save the PPO model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43bedc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'PPO_model_NEW3')\n",
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ce2fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda5b5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164f7300",
   "metadata": {},
   "source": [
    "## Testing the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63a663be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from CustomGymEnvSetup import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0880c257",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('instigo-goma-rl-v0',\n",
    "               #sumoconfig_file=\"E:/Instigo-TS-CTRL/network/osm.sumocfg\",\n",
    "               #sumoconfig_file=\"E:/Instigo-TS-CTRL/network/single-intersection-new.sumocfg\",\n",
    "               sumoconfig_file=\"E:/Instigo-TS-CTRL/network_trainning/single-intersection-new.sumocfg\",\n",
    "                #out_csv_name=\"./outputs/stat_NEW_test_FLUIDE2\",\n",
    "               fixed_ts = False,\n",
    "                use_gui=True,\n",
    "                num_seconds=5001,\n",
    "              )\n",
    "obs, info = env.reset()\n",
    "done = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb466ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "PPO_path = os.path.join('Training', 'PPO_model_NEW3')\n",
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3181f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 0.0,\n",
       " 'agent_total_vehicles_passed': [0],\n",
       " 'agent_total_fuel_consumption': [0.0],\n",
       " 'agent_co2_emission': [0.0],\n",
       " 'agent_accumulated_waiting_time': [0.0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db47b6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.0806162769202329, 0.08058739255014326]\n",
      "++++ Last Density :  0.0\n",
      "++++ NEW Density :  0.08060183473518809\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.08060183473518809\n",
      " Nb_Veh :  [3 3 3 3]\n",
      "\n",
      "++++ Phases density :  [0.09404750901192346, 0.13431232091690545]\n",
      "++++ Last Density :  0.08060183473518809\n",
      "++++ NEW Density :  0.11417991496441446\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.03357808022922637\n",
      " Nb_Veh :  [3 5 4 5]\n",
      "\n",
      "++++ Phases density :  [0.0806162769202329, 0.17460601719197705]\n",
      "++++ Last Density :  0.11417991496441446\n",
      "++++ NEW Density :  0.127611147056105\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013431232091690531\n",
      " Nb_Veh :  [3 7 3 6]\n",
      "\n",
      "++++ Phases density :  [0.12091960131866777, 0.16117478510028652]\n",
      "++++ Last Density :  0.127611147056105\n",
      "++++ NEW Density :  0.14104719320947715\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013436046153372155\n",
      " Nb_Veh :  [4 6 5 6]\n",
      "\n",
      "++++ Phases density :  [0.17466378593215637, 0.12088108882521491]\n",
      "++++ Last Density :  0.14104719320947715\n",
      "++++ NEW Density :  0.14777243737868564\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.0067252441692085\n",
      " Nb_Veh :  [6 4 7 5]\n",
      "\n",
      "++++ Phases density :  [0.12091960131866776, 0.10744985673352436]\n",
      "++++ Last Density :  0.14777243737868564\n",
      "++++ NEW Density :  0.11418472902609605\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258959\n",
      " Nb_Veh :  [4 4 5 4]\n",
      "\n",
      "++++ Phases density :  [0.0806162769202329, 0.16117478510028654]\n",
      "++++ Last Density :  0.11418472902609605\n",
      "++++ NEW Density :  0.12089553101025972\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.00671080198416367\n",
      " Nb_Veh :  [3 6 3 6]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.1880372492836676]\n",
      "++++ Last Density :  0.12089553101025972\n",
      "++++ NEW Density :  0.17463490156206674\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.05373937055180701\n",
      " Nb_Veh :  [6 7 6 7]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395443, 0.16117478510028652]\n",
      "++++ Last Density :  0.17463490156206674\n",
      "++++ NEW Density :  0.1880757617771205\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [8 5 8 7]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.14774355300859598]\n",
      "++++ Last Density :  0.1880757617771205\n",
      "++++ NEW Density :  0.15448805342453092\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [6 5 6 6]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.22833094555873926]\n",
      "++++ Last Density :  0.15448805342453092\n",
      "++++ NEW Density :  0.18134570354623042\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026857650121699495\n",
      " Nb_Veh :  [5 8 5 9]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.17460601719197705]\n",
      "++++ Last Density :  0.18134570354623042\n",
      "++++ NEW Density :  0.1544832393628493\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02686246418338112\n",
      " Nb_Veh :  [5 6 5 7]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.10744985673352436]\n",
      "++++ Last Density :  0.1544832393628493\n",
      "++++ NEW Density :  0.14777725144036724\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.0067059879224820595\n",
      " Nb_Veh :  [7 5 7 3]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.14774355300859598]\n",
      "++++ Last Density :  0.14777725144036724\n",
      "++++ NEW Density :  0.16792409957790305\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [7 7 7 4]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697722, 0.21489971346704873]\n",
      "++++ Last Density :  0.16792409957790305\n",
      "++++ NEW Density :  0.16119404134701298\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006730058230890068\n",
      " Nb_Veh :  [ 4 10  4  6]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.1746060171919771]\n",
      "++++ Last Density :  0.16119404134701298\n",
      "++++ NEW Density :  0.15448323936284933\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006710801984163656\n",
      " Nb_Veh :  [5 8 5 5]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.1208810888252149]\n",
      "++++ Last Density :  0.15448323936284933\n",
      "++++ NEW Density :  0.15449286748621252\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363192298e-06\n",
      " Nb_Veh :  [7 6 7 3]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.13431232091690543]\n",
      "++++ Last Density :  0.15449286748621252\n",
      "++++ NEW Density :  0.14777243737868564\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526876\n",
      " Nb_Veh :  [6 7 6 3]\n",
      "\n",
      "++++ Phases density :  [0.09405713713528668, 0.1880372492836676]\n",
      "++++ Last Density :  0.14777243737868564\n",
      "++++ NEW Density :  0.14104719320947715\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0067252441692085\n",
      " Nb_Veh :  [4 9 3 5]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.1880372492836676]\n",
      "++++ Last Density :  0.14104719320947715\n",
      "++++ NEW Density :  0.16791928551622146\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [6 9 5 5]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.16117478510028654]\n",
      "++++ Last Density :  0.16791928551622146\n",
      "++++ NEW Density :  0.18136014573127524\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.01344086021505378\n",
      " Nb_Veh :  [8 8 7 4]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.1880372492836676]\n",
      "++++ Last Density :  0.18136014573127524\n",
      "++++ NEW Density :  0.1813553316695936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [7 9 6 5]\n",
      "\n",
      "++++ Phases density :  [0.10747874110361401, 0.2417621776504298]\n",
      "++++ Last Density :  0.1813553316695936\n",
      "++++ NEW Density :  0.1746204593770219\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.00673487229257172\n",
      " Nb_Veh :  [ 3 11  5  7]\n",
      "\n",
      "++++ Phases density :  [0.10747874110361401, 0.1880372492836676]\n",
      "++++ Last Density :  0.1746204593770219\n",
      "++++ NEW Density :  0.1477579951936408\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02686246418338109\n",
      " Nb_Veh :  [3 9 5 5]\n",
      "\n",
      "++++ Phases density :  [0.18809501802384693, 0.13431232091690545]\n",
      "++++ Last Density :  0.1477579951936408\n",
      "++++ NEW Density :  0.16120366947037618\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013445674276735375\n",
      " Nb_Veh :  [6 7 8 3]\n",
      "\n",
      "++++ Phases density :  [0.18809501802384693, 0.18803724928366763]\n",
      "++++ Last Density :  0.16120366947037618\n",
      "++++ NEW Density :  0.1880661336537573\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02686246418338112\n",
      " Nb_Veh :  [6 9 8 5]\n",
      "\n",
      "++++ Phases density :  [0.13437008965708475, 0.2417621776504298]\n",
      "++++ Last Density :  0.1880661336537573\n",
      "++++ NEW Density :  0.18806613365375727\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  2.7755575615628914e-17\n",
      " Nb_Veh :  [ 6 11  4  7]\n",
      "\n",
      "++++ Phases density :  [0.13437008965708475, 0.1880372492836676]\n",
      "++++ Last Density :  0.18806613365375727\n",
      "++++ NEW Density :  0.16120366947037618\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02686246418338109\n",
      " Nb_Veh :  [6 9 4 5]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.14774355300859598]\n",
      "++++ Last Density :  0.16120366947037618\n",
      "++++ NEW Density :  0.18136495979295683\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020161290322580655\n",
      " Nb_Veh :  [9 7 7 4]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.14774355300859598]\n",
      "++++ Last Density :  0.18136495979295683\n",
      "++++ NEW Density :  0.15449286748621252\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02687209230674431\n",
      " Nb_Veh :  [7 7 5 4]\n",
      "\n",
      "++++ Phases density :  [0.10750762547370366, 0.20146848137535817]\n",
      "++++ Last Density :  0.15449286748621252\n",
      "++++ NEW Density :  0.15448805342453092\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.814061681596149e-06\n",
      " Nb_Veh :  [6 9 2 6]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.22833094555873926]\n",
      "++++ Last Density :  0.15448805342453092\n",
      "++++ NEW Density :  0.20150699386881105\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.04701894044428012\n",
      " Nb_Veh :  [ 8 10  5  7]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.14774355300859598]\n",
      "++++ Last Density :  0.20150699386881105\n",
      "++++ NEW Density :  0.18136977385463843\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02013722001417262\n",
      " Nb_Veh :  [10  8  6  3]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.14774355300859598]\n",
      "++++ Last Density :  0.18136977385463843\n",
      "++++ NEW Density :  0.1612132975937394\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02015647626089903\n",
      " Nb_Veh :  [8 7 5 4]\n",
      "\n",
      "++++ Phases density :  [0.13437008965708475, 0.20146848137535817]\n",
      "++++ Last Density :  0.1612132975937394\n",
      "++++ NEW Density :  0.16791928551622146\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.0067059879224820595\n",
      " Nb_Veh :  [6 9 4 6]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.14781094987213853, 0.1746060171919771]\n",
      "++++ Last Density :  0.16791928551622146\n",
      "++++ NEW Density :  0.1612084835320578\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006710801984163656\n",
      " Nb_Veh :  [7 8 4 5]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.13431232091690545]\n",
      "++++ Last Density :  0.1612084835320578\n",
      "++++ NEW Density :  0.1679337277012663\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.0067252441692085\n",
      " Nb_Veh :  [9 7 6 3]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.1880372492836676]\n",
      "++++ Last Density :  0.1679337277012663\n",
      "++++ NEW Density :  0.19479619188464736\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026862464183381063\n",
      " Nb_Veh :  [9 9 6 5]\n",
      "\n",
      "++++ Phases density :  [0.14781094987213853, 0.2417621776504298]\n",
      "++++ Last Density :  0.19479619188464736\n",
      "++++ NEW Density :  0.19478656376128417\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363192298e-06\n",
      " Nb_Veh :  [ 7 11  4  7]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.21489971346704873]\n",
      "++++ Last Density :  0.19478656376128417\n",
      "++++ NEW Density :  0.1880709477154389\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.00671561604584528\n",
      " Nb_Veh :  [ 7 10  5  6]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.1880372492836676]\n",
      "++++ Last Density :  0.1880709477154389\n",
      "++++ NEW Density :  0.20151180793049264\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [9 9 7 5]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.1880372492836676]\n",
      "++++ Last Density :  0.20151180793049264\n",
      "++++ NEW Density :  0.17463971562374833\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02687209230674431\n",
      " Nb_Veh :  [7 9 5 5]\n",
      "\n",
      "++++ Phases density :  [0.120929229442031, 0.2686246418338109]\n",
      "++++ Last Density :  0.17463971562374833\n",
      "++++ NEW Density :  0.19477693563792095\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02013722001417262\n",
      " Nb_Veh :  [ 5 12  4  8]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.2686246418338109]\n",
      "++++ Last Density :  0.19477693563792095\n",
      "++++ NEW Density :  0.22164902794466526\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 7 12  6  8]\n",
      "\n",
      "++++ Phases density :  [0.2284175986690082, 0.20146848137535817]\n",
      "++++ Last Density :  0.22164902794466526\n",
      "++++ NEW Density :  0.21494304002218317\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006705987922482087\n",
      " Nb_Veh :  [9 9 8 6]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.18803724928366763]\n",
      "++++ Last Density :  0.21494304002218317\n",
      "++++ NEW Density :  0.1813553316695936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [7 9 6 5]\n",
      "\n",
      "++++ Phases density :  [0.14781094987213853, 0.2686246418338109]\n",
      "++++ Last Density :  0.1813553316695936\n",
      "++++ NEW Density :  0.20821779585297473\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02686246418338112\n",
      " Nb_Veh :  [ 7 12  4  8]\n",
      "\n",
      "++++ Phases density :  [0.14781094987213853, 0.21489971346704873]\n",
      "++++ Last Density :  0.20821779585297473\n",
      "++++ NEW Density :  0.1813553316695936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02686246418338112\n",
      " Nb_Veh :  [ 7 10  4  6]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.16117478510028654]\n",
      "++++ Last Density :  0.1813553316695936\n",
      "++++ NEW Density :  0.18136495979295683\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363220054e-06\n",
      " Nb_Veh :  [9 8 6 4]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.2417621776504298]\n",
      "++++ Last Density :  0.18136495979295683\n",
      "++++ NEW Density :  0.2350947022214006\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.05372974242844378\n",
      " Nb_Veh :  [10 11  7  7]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.29548710601719197]\n",
      "++++ Last Density :  0.2350947022214006\n",
      "++++ NEW Density :  0.2350802600363558\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  1.4442185044816203e-05\n",
      " Nb_Veh :  [ 7 13  6  9]\n",
      "\n",
      "++++ Phases density :  [0.18811427427057337, 0.2417621776504298]\n",
      "++++ Last Density :  0.2350802600363558\n",
      "++++ NEW Density :  0.21493822596050158\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.020142034075854215\n",
      " Nb_Veh :  [ 8 11  6  7]\n",
      "\n",
      "++++ Phases density :  [0.26873055119080624, 0.2148997134670487]\n",
      "++++ Last Density :  0.21493822596050158\n",
      "++++ NEW Density :  0.24181513232892748\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026876906368425907\n",
      " Nb_Veh :  [11 10  9  6]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.21489971346704873]\n",
      "++++ Last Density :  0.24181513232892748\n",
      "++++ NEW Density :  0.2149430400221832\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744283\n",
      " Nb_Veh :  [ 9 10  7  6]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.2686246418338109]\n",
      "++++ Last Density :  0.2149430400221832\n",
      "++++ NEW Density :  0.22165384200634686\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006710801984163656\n",
      " Nb_Veh :  [ 8 12  5  8]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.2686246418338109]\n",
      "++++ Last Density :  0.22165384200634686\n",
      "++++ NEW Density :  0.235089888159719\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013436046153372155\n",
      " Nb_Veh :  [ 9 12  6  8]\n",
      "\n",
      "++++ Phases density :  [0.25529931909911574, 0.21489971346704873]\n",
      "++++ Last Density :  0.235089888159719\n",
      "++++ NEW Density :  0.23509951628308223\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363220054e-06\n",
      " Nb_Veh :  [11 10  8  6]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.21489971346704873]\n",
      "++++ Last Density :  0.23509951628308223\n",
      "++++ NEW Density :  0.22166347012971008\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372155\n",
      " Nb_Veh :  [10 10  7  6]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.2686246418338109]\n",
      "++++ Last Density :  0.22166347012971008\n",
      "++++ NEW Density :  0.22165384200634686\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363220054e-06\n",
      " Nb_Veh :  [ 8 12  5  8]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.21489971346704873]\n",
      "++++ Last Density :  0.22165384200634686\n",
      "++++ NEW Density :  0.1880757617771205\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.03357808022922637\n",
      " Nb_Veh :  [ 8 10  4  6]\n",
      "\n",
      "++++ Phases density :  [0.24186808700742518, 0.20146848137535817]\n",
      "++++ Last Density :  0.1880757617771205\n",
      "++++ NEW Density :  0.22166828419139167\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.033592522414271186\n",
      " Nb_Veh :  [11 10  7  5]\n",
      "\n",
      "++++ Phases density :  [0.24186808700742518, 0.24176217765042982]\n",
      "++++ Last Density :  0.22166828419139167\n",
      "++++ NEW Density :  0.24181513232892748\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [11 12  7  6]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.2820558739255014]\n",
      "++++ Last Density :  0.24181513232892748\n",
      "++++ NEW Density :  0.2283694580521921\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013445674276735375\n",
      " Nb_Veh :  [ 8 14  5  7]\n",
      "\n",
      "++++ Phases density :  [0.13437008965708475, 0.3223495702005731]\n",
      "++++ Last Density :  0.2283694580521921\n",
      "++++ NEW Density :  0.22835982992882892\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363192298e-06\n",
      " Nb_Veh :  [ 6 15  4  9]\n",
      "\n",
      "++++ Phases density :  [0.120929229442031, 0.3760744985673352]\n",
      "++++ Last Density :  0.22835982992882892\n",
      "++++ NEW Density :  0.2485018640046831\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020142034075854187\n",
      " Nb_Veh :  [ 5 17  4 11]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.3760744985673352]\n",
      "++++ Last Density :  0.2485018640046831\n",
      "++++ NEW Density :  0.2753739563114274\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 7 17  6 11]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.28205587392550147]\n",
      "++++ Last Density :  0.2753739563114274\n",
      "++++ NEW Density :  0.24180069014388267\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.033573266167544746\n",
      " Nb_Veh :  [ 8 13  7  8]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.3223495702005731]\n",
      "++++ Last Density :  0.24180069014388267\n",
      "++++ NEW Density :  0.2619475382814185\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [ 8 15  7  9]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.3760744985673352]\n",
      "++++ Last Density :  0.2619475382814185\n",
      "++++ NEW Density :  0.2686583402655821\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006710801984163628\n",
      " Nb_Veh :  [ 7 17  5 11]\n",
      "\n",
      "++++ Phases density :  [0.09405713713528668, 0.42979942693409745]\n",
      "++++ Last Density :  0.2686583402655821\n",
      "++++ NEW Density :  0.2619282820346921\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0067300582308900125\n",
      " Nb_Veh :  [ 4 19  3 13]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.40293696275071633]\n",
      "++++ Last Density :  0.2619282820346921\n",
      "++++ NEW Density :  0.27536914224974585\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [ 6 18  5 12]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.3357808022922636]\n",
      "++++ Last Density :  0.27536914224974585\n",
      "++++ NEW Density :  0.26866315432726373\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006705987922482115\n",
      " Nb_Veh :  [ 8 16  7  9]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.3357808022922636]\n",
      "++++ Last Density :  0.26866315432726373\n",
      "++++ NEW Density :  0.24179106202051945\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744283\n",
      " Nb_Veh :  [ 6 16  5  9]\n",
      "\n",
      "++++ Phases density :  [0.12093885756539419, 0.4163681948424069]\n",
      "++++ Last Density :  0.24179106202051945\n",
      "++++ NEW Density :  0.26865352620390054\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02686246418338109\n",
      " Nb_Veh :  [ 6 19  3 12]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.1746830421788828, 0.4163681948424069]\n",
      "++++ Last Density :  0.26865352620390054\n",
      "++++ NEW Density :  0.29552561851064485\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 8 19  5 12]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.3626432664756447]\n",
      "++++ Last Density :  0.29552561851064485\n",
      "++++ NEW Density :  0.2955352466340081\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363247809e-06\n",
      " Nb_Veh :  [10 17  7 10]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.3492120343839542]\n",
      "++++ Last Density :  0.2955352466340081\n",
      "++++ NEW Density :  0.2619475382814185\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [ 8 16  5 10]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.42979942693409745]\n",
      "++++ Last Density :  0.2619475382814185\n",
      "++++ NEW Density :  0.29552561851064485\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.03357808022922637\n",
      " Nb_Veh :  [ 8 19  4 13]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.48352435530085963]\n",
      "++++ Last Density :  0.29552561851064485\n",
      "++++ NEW Density :  0.32238808269402597\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02686246418338112\n",
      " Nb_Veh :  [ 8 21  4 15]\n",
      "\n",
      "++++ Phases density :  [0.08062590504359614, 0.48352435530085963]\n",
      "++++ Last Density :  0.32238808269402597\n",
      "++++ NEW Density :  0.2820751301722279\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04031295252179806\n",
      " Nb_Veh :  [ 4 21  2 15]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.5103868194842407]\n",
      "++++ Last Density :  0.2820751301722279\n",
      "++++ NEW Density :  0.32909407061650797\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.047018940444280066\n",
      " Nb_Veh :  [ 6 22  5 16]\n",
      "\n",
      "++++ Phases density :  [0.20154550636226393, 0.443230659025788]\n",
      "++++ Last Density :  0.32909407061650797\n",
      "++++ NEW Density :  0.32238808269402597\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006705987922482004\n",
      " Nb_Veh :  [ 8 20  7 13]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.42979942693409745]\n",
      "++++ Last Density :  0.32238808269402597\n",
      "++++ NEW Density :  0.28880037434143635\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [ 6 19  5 13]\n",
      "\n",
      "++++ Phases density :  [0.12093885756539421, 0.5103868194842407]\n",
      "++++ Last Density :  0.28880037434143635\n",
      "++++ NEW Density :  0.31566283852481747\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02686246418338112\n",
      " Nb_Veh :  [ 6 22  3 16]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.5103868194842407]\n",
      "++++ Last Density :  0.31566283852481747\n",
      "++++ NEW Density :  0.3425349308315617\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026872092306744255\n",
      " Nb_Veh :  [ 8 22  5 16]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.3895057306590258]\n",
      "++++ Last Density :  0.3425349308315617\n",
      "++++ NEW Density :  0.2820943864189543\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.06044054441260743\n",
      " Nb_Veh :  [ 8 17  5 12]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.42979942693409745]\n",
      "++++ Last Density :  0.2820943864189543\n",
      "++++ NEW Density :  0.3022412345564901\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [ 8 19  5 13]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224603, 0.5103868194842407]\n",
      "++++ Last Density :  0.3022412345564901\n",
      "++++ NEW Density :  0.34253974489324335\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.040298510336753246\n",
      " Nb_Veh :  [ 9 22  4 16]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.5641117478510029]\n",
      "++++ Last Density :  0.34253974489324335\n",
      "++++ NEW Density :  0.3559565347998891\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013416789906645743\n",
      " Nb_Veh :  [ 6 24  5 18]\n",
      "\n",
      "++++ Phases density :  [0.05374418461348861, 0.5641117478510029]\n",
      "++++ Last Density :  0.3559565347998891\n",
      "++++ NEW Density :  0.3089279662322458\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.047028568567643314\n",
      " Nb_Veh :  [ 2 24  2 18]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.5909742120343839]\n",
      "++++ Last Density :  0.3089279662322458\n",
      "++++ NEW Density :  0.3626673367840527\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.05373937055180694\n",
      " Nb_Veh :  [ 5 25  5 19]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721017, 0.5372492836676217]\n",
      "++++ Last Density :  0.3626673367840527\n",
      "++++ NEW Density :  0.36267696490741597\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -9.628123363247809e-06\n",
      " Nb_Veh :  [ 7 22  7 18]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372153, 0.5372492836676217]\n",
      "++++ Last Density :  0.36267696490741597\n",
      "++++ NEW Density :  0.3358048726006716\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744366\n",
      " Nb_Veh :  [ 5 22  5 18]\n",
      "\n",
      "++++ Phases density :  [0.09405713713528668, 0.5909742120343839]\n",
      "++++ Last Density :  0.3358048726006716\n",
      "++++ NEW Density :  0.34251567458483534\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006710801984163739\n",
      " Nb_Veh :  [ 4 24  3 20]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.6044054441260744]\n",
      "++++ Last Density :  0.34251567458483534\n",
      "++++ NEW Density :  0.37610338293742485\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03358770835258951\n",
      " Nb_Veh :  [ 6 25  5 20]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.4969555873925502]\n",
      "++++ Last Density :  0.37610338293742485\n",
      "++++ NEW Density :  0.3358145007240349\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04028888221338994\n",
      " Nb_Veh :  [ 7 21  6 16]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.5372492836676217]\n",
      "++++ Last Density :  0.3358145007240349\n",
      "++++ NEW Density :  0.35596134886157066\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020146848137535756\n",
      " Nb_Veh :  [ 7 22  6 18]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.6178366762177651]\n",
      "++++ Last Density :  0.35596134886157066\n",
      "++++ NEW Density :  0.37609856887574333\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020137220014172674\n",
      " Nb_Veh :  [ 5 25  5 21]\n",
      "\n",
      "++++ Phases density :  [0.12091960131866777, 0.5641117478510029]\n",
      "++++ Last Density :  0.37609856887574333\n",
      "++++ NEW Density :  0.34251567458483534\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.033582894290907994\n",
      " Nb_Veh :  [ 4 23  5 19]\n",
      "\n",
      "++++ Phases density :  [0.17466378593215637, 0.4835243553008596]\n",
      "++++ Last Density :  0.34251567458483534\n",
      "++++ NEW Density :  0.32909407061650797\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.013421603968327367\n",
      " Nb_Veh :  [ 6 20  7 16]\n",
      "\n",
      "++++ Phases density :  [0.17466378593215637, 0.5372492836676217]\n",
      "++++ Last Density :  0.32909407061650797\n",
      "++++ NEW Density :  0.35595653479988904\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026862464183381063\n",
      " Nb_Veh :  [ 6 22  7 18]\n",
      "\n",
      "++++ Phases density :  [0.09404750901192346, 0.5641117478510029]\n",
      "++++ Last Density :  0.35595653479988904\n",
      "++++ NEW Density :  0.3290796284314632\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026876906368425824\n",
      " Nb_Veh :  [ 3 23  4 19]\n",
      "\n",
      "++++ Phases density :  [0.14779169362541208, 0.5641117478510029]\n",
      "++++ Last Density :  0.3290796284314632\n",
      "++++ NEW Density :  0.3559517207382075\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 5 23  6 19]\n",
      "\n",
      "++++ Phases density :  [0.20153587823890068, 0.5372492836676217]\n",
      "++++ Last Density :  0.3559517207382075\n",
      "++++ NEW Density :  0.3693925809532612\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013440860215053696\n",
      " Nb_Veh :  [ 7 22  8 18]\n",
      "\n",
      "++++ Phases density :  [0.17466378593215637, 0.5641117478510029]\n",
      "++++ Last Density :  0.3693925809532612\n",
      "++++ NEW Density :  0.36938776689157965\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616815683934e-06\n",
      " Nb_Veh :  [ 6 23  7 19]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697722, 0.6178366762177651]\n",
      "++++ Last Density :  0.36938776689157965\n",
      "++++ NEW Density :  0.36266252272237115\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0067252441692085\n",
      " Nb_Veh :  [ 4 25  4 21]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.6044054441260744]\n",
      "++++ Last Density :  0.36266252272237115\n",
      "++++ NEW Density :  0.38281899898327015\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899003\n",
      " Nb_Veh :  [ 6 25  6 20]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.4969555873925502]\n",
      "++++ Last Density :  0.38281899898327015\n",
      "++++ NEW Density :  0.329094070616508\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.053724928366762126\n",
      " Nb_Veh :  [ 6 21  6 16]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.5775429799426934]\n",
      "++++ Last Density :  0.329094070616508\n",
      "++++ NEW Density :  0.3828238130449518\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.05372974242844375\n",
      " Nb_Veh :  [ 7 24  7 19]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.14779169362541208, 0.6312679083094557]\n",
      "++++ Last Density :  0.3828238130449518\n",
      "++++ NEW Density :  0.3895298009674339\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006705987922482115\n",
      " Nb_Veh :  [ 5 26  6 21]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.5775429799426934]\n",
      "++++ Last Density :  0.3895298009674339\n",
      "++++ NEW Density :  0.35595172073820747\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.033578080229226426\n",
      " Nb_Veh :  [ 5 24  5 19]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395443, 0.5372492836676217]\n",
      "++++ Last Density :  0.35595172073820747\n",
      "++++ NEW Density :  0.3761130110607881\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020161290322580627\n",
      " Nb_Veh :  [ 8 22  8 18]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.5506805157593123]\n",
      "++++ Last Density :  0.3761130110607881\n",
      "++++ NEW Density :  0.3693925809532612\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526876\n",
      " Nb_Veh :  [ 7 23  7 18]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697722, 0.5641117478510029]\n",
      "++++ Last Density :  0.3693925809532612\n",
      "++++ NEW Density :  0.3358000585389901\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03359252241427113\n",
      " Nb_Veh :  [ 4 23  4 19]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.5641117478510029]\n",
      "++++ Last Density :  0.3358000585389901\n",
      "++++ NEW Density :  0.3626721508457344\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 6 23  6 19]\n",
      "\n",
      "++++ Phases density :  [0.24184883076069874, 0.5372492836676217]\n",
      "++++ Last Density :  0.3626721508457344\n",
      "++++ NEW Density :  0.3895490572141602\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026876906368425824\n",
      " Nb_Veh :  [ 9 22  9 18]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.5238180515759312]\n",
      "++++ Last Density :  0.3895490572141602\n",
      "++++ NEW Density :  0.35596134886157066\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [ 7 22  7 17]\n",
      "\n",
      "++++ Phases density :  [0.12091960131866777, 0.5775429799426934]\n",
      "++++ Last Density :  0.35596134886157066\n",
      "++++ NEW Density :  0.3492312906306806\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006730058230890068\n",
      " Nb_Veh :  [ 4 24  5 19]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.5775429799426934]\n",
      "++++ Last Density :  0.3492312906306806\n",
      "++++ NEW Density :  0.36938776689157965\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02015647626089906\n",
      " Nb_Veh :  [ 6 24  6 19]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.48352435530085963]\n",
      "++++ Last Density :  0.36938776689157965\n",
      "++++ NEW Density :  0.3358145007240349\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.033573266167544746\n",
      " Nb_Veh :  [ 7 21  7 15]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.5372492836676217]\n",
      "++++ Last Density :  0.3358145007240349\n",
      "++++ NEW Density :  0.3626769649074159\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026862464183381007\n",
      " Nb_Veh :  [ 7 23  7 17]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.5909742120343839]\n",
      "++++ Last Density :  0.3626769649074159\n",
      "++++ NEW Density :  0.3693877668915796\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.0067108019841636835\n",
      " Nb_Veh :  [ 6 25  5 19]\n",
      "\n",
      "++++ Phases density :  [0.0806162769202329, 0.6178366762177652]\n",
      "++++ Last Density :  0.3693877668915796\n",
      "++++ NEW Density :  0.349226476568999\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.020161290322580572\n",
      " Nb_Veh :  [ 3 26  3 20]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.6178366762177651]\n",
      "++++ Last Density :  0.349226476568999\n",
      "++++ NEW Density :  0.37609856887574333\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 5 26  5 20]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.5641117478510029]\n",
      "++++ Last Density :  0.37609856887574333\n",
      "++++ NEW Density :  0.3761081969991065\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -9.628123363192298e-06\n",
      " Nb_Veh :  [ 7 24  7 18]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.5909742120343839]\n",
      "++++ Last Density :  0.3761081969991065\n",
      "++++ NEW Density :  0.3761033829374249\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [ 6 25  6 19]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697723, 0.6446991404011462]\n",
      "++++ Last Density :  0.3761033829374249\n",
      "++++ NEW Density :  0.3760937548140617\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363192298e-06\n",
      " Nb_Veh :  [ 4 27  4 21]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.6446991404011462]\n",
      "++++ Last Density :  0.3760937548140617\n",
      "++++ NEW Density :  0.402965847120806\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687209230674431\n",
      " Nb_Veh :  [ 6 27  6 21]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.5372492836676217]\n",
      "++++ Last Density :  0.402965847120806\n",
      "++++ NEW Density :  0.3492409187540438\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.05372492836676224\n",
      " Nb_Veh :  [ 6 22  6 18]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.6178366762177651]\n",
      "++++ Last Density :  0.3492409187540438\n",
      "++++ NEW Density :  0.4029706611824876\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.053729742428443805\n",
      " Nb_Veh :  [ 7 25  7 21]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.6715616045845272]\n",
      "++++ Last Density :  0.4029706611824876\n",
      "++++ NEW Density :  0.4163970792124965\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013426418030008935\n",
      " Nb_Veh :  [ 6 27  6 23]\n",
      "\n",
      "++++ Phases density :  [0.14779169362541208, 0.7118553008595989]\n",
      "++++ Last Density :  0.4163970792124965\n",
      "++++ NEW Density :  0.4298234972425055\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.01342641803000899\n",
      " Nb_Veh :  [ 5 29  6 24]\n",
      "\n",
      "++++ Phases density :  [0.1612325538404658, 0.6984240687679083]\n",
      "++++ Last Density :  0.4298234972425055\n",
      "++++ NEW Density :  0.4298283113041871\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -4.8140616815683934e-06\n",
      " Nb_Veh :  [ 6 28  6 24]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395443, 0.6581303724928367]\n",
      "++++ Last Density :  0.4298283113041871\n",
      "++++ NEW Density :  0.4365535554733956\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.0067252441692085\n",
      " Nb_Veh :  [ 8 28  8 21]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.6581303724928367]\n",
      "++++ Last Density :  0.4365535554733956\n",
      "++++ NEW Density :  0.40968146316665127\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02687209230674431\n",
      " Nb_Veh :  [ 6 28  6 21]\n",
      "\n",
      "++++ Phases density :  [0.120929229442031, 0.7118553008595989]\n",
      "++++ Last Density :  0.40968146316665127\n",
      "++++ NEW Density :  0.41639226515081496\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.0067108019841636835\n",
      " Nb_Veh :  [ 5 30  4 23]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.7387177650429799]\n",
      "++++ Last Density :  0.41639226515081496\n",
      "++++ NEW Density :  0.4701316357026219\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.05373937055180694\n",
      " Nb_Veh :  [ 8 31  7 24]\n",
      "\n",
      "++++ Phases density :  [0.2552896909757525, 0.6849928366762177]\n",
      "++++ Last Density :  0.4701316357026219\n",
      "++++ NEW Density :  0.4701412638259851\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363192298e-06\n",
      " Nb_Veh :  [10 29  9 22]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.6715616045845272]\n",
      "++++ Last Density :  0.4701412638259851\n",
      "++++ NEW Density :  0.4365535554733955\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [ 8 29  7 21]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7387177650429799]\n",
      "++++ Last Density :  0.4365535554733955\n",
      "++++ NEW Density :  0.463411205595095\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026857650121699495\n",
      " Nb_Veh :  [ 7 31  7 24]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046584, 0.8058739255014327]\n",
      "++++ Last Density :  0.463411205595095\n",
      "++++ NEW Density :  0.48355323967094926\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020142034075854243\n",
      " Nb_Veh :  [ 6 34  6 26]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.7521489971346705]\n",
      "++++ Last Density :  0.48355323967094926\n",
      "++++ NEW Density :  0.44997515944172295\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.033578080229226315\n",
      " Nb_Veh :  [ 6 32  5 24]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.6849928366762177]\n",
      "++++ Last Density :  0.44997515944172295\n",
      "++++ NEW Density :  0.4432691715192408\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006705987922482171\n",
      " Nb_Veh :  [ 8 29  7 22]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.7118553008595989]\n",
      "++++ Last Density :  0.4432691715192408\n",
      "++++ NEW Density :  0.4567004036109314\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690615\n",
      " Nb_Veh :  [ 8 29  7 24]\n",
      "\n",
      "++++ Phases density :  [0.120929229442031, 0.7387177650429799]\n",
      "++++ Last Density :  0.4567004036109314\n",
      "++++ NEW Density :  0.42982349724250546\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026876906368425935\n",
      " Nb_Veh :  [ 5 30  4 25]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.7521489971346704]\n",
      "++++ Last Density :  0.42982349724250546\n",
      "++++ NEW Density :  0.463411205595095\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03358770835258956\n",
      " Nb_Veh :  [ 7 31  6 25]\n",
      "\n",
      "++++ Phases density :  [0.25528969097575255, 0.7118553008595989]\n",
      "++++ Last Density :  0.463411205595095\n",
      "++++ NEW Density :  0.4835724959176757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020161290322580683\n",
      " Nb_Veh :  [10 29  9 24]\n",
      "\n",
      "++++ Phases density :  [0.20154550636226393, 0.6984240687679083]\n",
      "++++ Last Density :  0.4835724959176757\n",
      "++++ NEW Density :  0.4499847875650861\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [ 8 29  7 23]\n",
      "\n",
      "++++ Phases density :  [0.14779169362541208, 0.7521489971346704]\n",
      "++++ Last Density :  0.4499847875650861\n",
      "++++ NEW Density :  0.44997034538004127\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  1.4442185044816203e-05\n",
      " Nb_Veh :  [ 5 31  6 25]\n",
      "\n",
      "++++ Phases density :  [0.18809501802384693, 0.7521489971346705]\n",
      "++++ Last Density :  0.44997034538004127\n",
      "++++ NEW Density :  0.4701220075792587\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020151662199217435\n",
      " Nb_Veh :  [ 6 32  8 24]\n",
      "\n",
      "++++ Phases density :  [0.21496711033059124, 0.6581303724928367]\n",
      "++++ Last Density :  0.4701220075792587\n",
      "++++ NEW Density :  0.43654874141171396\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.033573266167544746\n",
      " Nb_Veh :  [ 7 28  9 21]\n",
      "\n",
      "++++ Phases density :  [0.21496711033059124, 0.6984240687679083]\n",
      "++++ Last Density :  0.43654874141171396\n",
      "++++ NEW Density :  0.45669558954924977\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [ 7 30  9 22]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.7387177650429799]\n",
      "++++ Last Density :  0.45669558954924977\n",
      "++++ NEW Density :  0.44997997350340446\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006715616045845307\n",
      " Nb_Veh :  [ 7 31  5 24]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.7252865329512894]\n",
      "++++ Last Density :  0.44997997350340446\n",
      "++++ NEW Density :  0.4432643574575592\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006715616045845252\n",
      " Nb_Veh :  [ 7 31  5 23]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.6984240687679083]\n",
      "++++ Last Density :  0.4432643574575592\n",
      "++++ NEW Density :  0.45670521767261296\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [ 9 30  7 22]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.7252865329512894]\n",
      "++++ Last Density :  0.45670521767261296\n",
      "++++ NEW Density :  0.4701364497643036\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690615\n",
      " Nb_Veh :  [ 9 30  7 24]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697722, 0.7387177650429799]\n",
      "++++ Last Density :  0.4701364497643036\n",
      "++++ NEW Density :  0.4231030671349786\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.047033382629324993\n",
      " Nb_Veh :  [ 4 31  4 24]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7521489971346704]\n",
      "++++ Last Density :  0.4231030671349786\n",
      "++++ NEW Density :  0.4701268216409403\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.04702375450596169\n",
      " Nb_Veh :  [ 7 31  7 25]\n",
      "\n",
      "++++ Phases density :  [0.24184883076069874, 0.6849928366762178]\n",
      "++++ Last Density :  0.4701268216409403\n",
      "++++ NEW Density :  0.46342083371845827\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006705987922482004\n",
      " Nb_Veh :  [ 9 29  9 22]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.6715616045845273]\n",
      "++++ Last Density :  0.46342083371845827\n",
      "++++ NEW Density :  0.4298331253658687\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [ 7 29  7 21]\n",
      "\n",
      "++++ Phases density :  [0.16122292571710262, 0.7387177650429799]\n",
      "++++ Last Density :  0.4298331253658687\n",
      "++++ NEW Density :  0.44997034538004127\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020137220014172563\n",
      " Nb_Veh :  [ 5 31  7 24]\n",
      "\n",
      "++++ Phases density :  [0.17466378593215637, 0.7252865329512894]\n",
      "++++ Last Density :  0.44997034538004127\n",
      "++++ NEW Density :  0.4499751594417229\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -4.8140616816239046e-06\n",
      " Nb_Veh :  [ 6 31  7 23]\n",
      "\n",
      "++++ Phases density :  [0.20153587823890068, 0.6446991404011462]\n",
      "++++ Last Density :  0.4499751594417229\n",
      "++++ NEW Density :  0.42311750932002345\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02685765012169944\n",
      " Nb_Veh :  [ 7 29  8 19]\n",
      "\n",
      "++++ Phases density :  [0.20153587823890068, 0.6715616045845272]\n",
      "++++ Last Density :  0.42311750932002345\n",
      "++++ NEW Density :  0.43654874141171396\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690504\n",
      " Nb_Veh :  [ 7 29  8 21]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7387177650429799]\n",
      "++++ Last Density :  0.43654874141171396\n",
      "++++ NEW Density :  0.463411205595095\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026862464183381063\n",
      " Nb_Veh :  [ 7 31  7 24]\n",
      "\n",
      "++++ Phases density :  [0.09405713713528668, 0.765580229226361]\n",
      "++++ Last Density :  0.463411205595095\n",
      "++++ NEW Density :  0.42981868318082384\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.033592522414271186\n",
      " Nb_Veh :  [ 4 33  3 24]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.765580229226361]\n",
      "++++ Last Density :  0.42981868318082384\n",
      "++++ NEW Density :  0.4499703453800413\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02015166219921749\n",
      " Nb_Veh :  [ 5 33  5 24]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395446, 0.7118553008595989]\n",
      "++++ Last Density :  0.4499703453800413\n",
      "++++ NEW Density :  0.4634160196567767\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013445674276735375\n",
      " Nb_Veh :  [ 8 30  8 23]\n",
      "\n",
      "++++ Phases density :  [0.1612325538404658, 0.7118553008595989]\n",
      "++++ Last Density :  0.4634160196567767\n",
      "++++ NEW Density :  0.43654392735003233\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744366\n",
      " Nb_Veh :  [ 6 30  6 23]\n",
      "\n",
      "++++ Phases density :  [0.12091960131866777, 0.7387177650429799]\n",
      "++++ Last Density :  0.43654392735003233\n",
      "++++ NEW Density :  0.42981868318082384\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.0067252441692085\n",
      " Nb_Veh :  [ 4 30  5 25]\n",
      "\n",
      "++++ Phases density :  [0.16122292571710262, 0.7521489971346704]\n",
      "++++ Last Density :  0.42981868318082384\n",
      "++++ NEW Density :  0.4566859614258865\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026867278245062687\n",
      " Nb_Veh :  [ 5 31  7 25]\n",
      "\n",
      "++++ Phases density :  [0.18809501802384693, 0.6715616045845272]\n",
      "++++ Last Density :  0.4566859614258865\n",
      "++++ NEW Density :  0.4298283113041871\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02685765012169944\n",
      " Nb_Veh :  [ 6 28  8 22]\n",
      "\n",
      "++++ Phases density :  [0.18809501802384693, 0.6849928366762178]\n",
      "++++ Last Density :  0.4298283113041871\n",
      "++++ NEW Density :  0.43654392735003233\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006715616045845252\n",
      " Nb_Veh :  [ 6 28  8 23]\n",
      "\n",
      "++++ Phases density :  [0.13436046153372155, 0.7387177650429799]\n",
      "++++ Last Density :  0.43654392735003233\n",
      "++++ NEW Density :  0.4365391132883507\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [ 5 30  5 25]\n",
      "\n",
      "++++ Phases density :  [0.1612325538404658, 0.7252865329512894]\n",
      "++++ Last Density :  0.4365391132883507\n",
      "++++ NEW Density :  0.4432595433958776\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006720430107526876\n",
      " Nb_Veh :  [ 6 30  6 24]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395443, 0.6984240687679083]\n",
      "++++ Last Density :  0.4432595433958776\n",
      "++++ NEW Density :  0.45670040361093134\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [ 8 29  8 23]\n",
      "\n",
      "++++ Phases density :  [0.21497673845395443, 0.7252865329512894]\n",
      "++++ Last Density :  0.45670040361093134\n",
      "++++ NEW Density :  0.47013163570262195\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690615\n",
      " Nb_Veh :  [ 8 29  8 25]\n",
      "\n",
      "++++ Phases density :  [0.10748836922697723, 0.7387177650429799]\n",
      "++++ Last Density :  0.47013163570262195\n",
      "++++ NEW Density :  0.4231030671349786\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04702856856764337\n",
      " Nb_Veh :  [ 4 30  4 25]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7521489971346705]\n",
      "++++ Last Density :  0.4231030671349786\n",
      "++++ NEW Density :  0.47012682164094033\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.047023754505961746\n",
      " Nb_Veh :  [ 7 30  7 26]\n",
      "\n",
      "++++ Phases density :  [0.24184883076069874, 0.6984240687679083]\n",
      "++++ Last Density :  0.47012682164094033\n",
      "++++ NEW Density :  0.4701364497643035\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363192298e-06\n",
      " Nb_Veh :  [ 9 30  9 22]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.6984240687679083]\n",
      "++++ Last Density :  0.4701364497643035\n",
      "++++ NEW Density :  0.4432643574575592\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02687209230674431\n",
      " Nb_Veh :  [ 7 30  7 22]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.1612325538404658, 0.765580229226361]\n",
      "++++ Last Density :  0.4432643574575592\n",
      "++++ NEW Density :  0.46340639153341345\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020142034075854243\n",
      " Nb_Veh :  [ 6 32  6 25]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.765580229226361]\n",
      "++++ Last Density :  0.46340639153341345\n",
      "++++ NEW Density :  0.4768424376867856\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013436046153372128\n",
      " Nb_Veh :  [ 7 32  7 25]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.6715616045845272]\n",
      "++++ Last Density :  0.4768424376867856\n",
      "++++ NEW Density :  0.42983312536586865\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04700931232091693\n",
      " Nb_Veh :  [ 7 30  7 20]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7118553008595989]\n",
      "++++ Last Density :  0.42983312536586865\n",
      "++++ NEW Density :  0.4499799735034045\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020146848137535867\n",
      " Nb_Veh :  [ 7 31  7 22]\n",
      "\n",
      "++++ Phases density :  [0.18810464614721015, 0.7790114613180515]\n",
      "++++ Last Density :  0.4499799735034045\n",
      "++++ NEW Density :  0.48355805373263083\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.033578080229226315\n",
      " Nb_Veh :  [ 7 33  7 25]\n",
      "\n",
      "++++ Phases density :  [0.16123255384046586, 0.8327363896848138]\n",
      "++++ Last Density :  0.48355805373263083\n",
      "++++ NEW Density :  0.4969844717626398\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.01342641803000899\n",
      " Nb_Veh :  [ 6 35  6 27]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.7924426934097422]\n",
      "++++ Last Density :  0.4969844717626398\n",
      "++++ NEW Density :  0.4701220075792587\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02686246418338112\n",
      " Nb_Veh :  [ 6 34  5 25]\n",
      "\n",
      "++++ Phases density :  [0.2284175986690082, 0.73871776504298]\n",
      "++++ Last Density :  0.4701220075792587\n",
      "++++ NEW Density :  0.48356768185599414\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013445674276735431\n",
      " Nb_Veh :  [ 9 29  8 26]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.7252865329512894]\n",
      "++++ Last Density :  0.48356768185599414\n",
      "++++ NEW Density :  0.4499799735034045\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [ 7 28  6 26]\n",
      "\n",
      "++++ Phases density :  [0.10750762547370366, 0.765580229226361]\n",
      "++++ Last Density :  0.4499799735034045\n",
      "++++ NEW Density :  0.43654392735003233\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372183\n",
      " Nb_Veh :  [ 6 29  2 28]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.7790114613180517]\n",
      "++++ Last Density :  0.43654392735003233\n",
      "++++ NEW Density :  0.47013163570262195\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03358770835258962\n",
      " Nb_Veh :  [ 8 30  4 28]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.7521489971346704]\n",
      "++++ Last Density :  0.47013163570262195\n",
      "++++ NEW Density :  0.48357249591767565\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053696\n",
      " Nb_Veh :  [10 30  6 26]\n",
      "\n",
      "++++ Phases density :  [0.1881239023939366, 0.765580229226361]\n",
      "++++ Last Density :  0.48357249591767565\n",
      "++++ NEW Density :  0.47685206581014883\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.00672043010752682\n",
      " Nb_Veh :  [ 9 30  5 27]\n",
      "\n",
      "++++ Phases density :  [0.13438934590381119, 0.8058739255014327]\n",
      "++++ Last Density :  0.47685206581014883\n",
      "++++ NEW Density :  0.47013163570262195\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526876\n",
      " Nb_Veh :  [ 8 31  2 29]\n",
      "\n",
      "++++ Phases density :  [0.13437971778044797, 0.792442693409742]\n",
      "++++ Last Density :  0.47013163570262195\n",
      "++++ NEW Density :  0.463411205595095\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006720430107526931\n",
      " Nb_Veh :  [ 7 31  3 28]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.765580229226361]\n",
      "++++ Last Density :  0.463411205595095\n",
      "++++ NEW Density :  0.4835724959176757\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020161290322580683\n",
      " Nb_Veh :  [10 30  5 27]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.7655802292263609]\n",
      "++++ Last Density :  0.4835724959176757\n",
      "++++ NEW Density :  0.48357249591767565\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  5.551115123125783e-17\n",
      " Nb_Veh :  [10 29  5 28]\n",
      "\n",
      "++++ Phases density :  [0.13437971778044797, 0.8058739255014327]\n",
      "++++ Last Density :  0.48357249591767565\n",
      "++++ NEW Density :  0.47012682164094033\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.01344567427673532\n",
      " Nb_Veh :  [ 7 30  3 30]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224603, 0.7924426934097422]\n",
      "++++ Last Density :  0.47012682164094033\n",
      "++++ NEW Density :  0.4835676818559941\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [ 9 30  4 29]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573462, 0.765580229226361]\n",
      "++++ Last Density :  0.4835676818559941\n",
      "++++ NEW Density :  0.49700854207104783\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [11 30  6 27]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224606, 0.7521489971346704]\n",
      "++++ Last Density :  0.49700854207104783\n",
      "++++ NEW Density :  0.4634208337184582\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [ 9 29  4 27]\n",
      "\n",
      "++++ Phases density :  [0.14782057799550172, 0.8058739255014327]\n",
      "++++ Last Density :  0.4634208337184582\n",
      "++++ NEW Density :  0.4768472517484672\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.01342641803000899\n",
      " Nb_Veh :  [ 8 30  3 30]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.8193051575931232]\n",
      "++++ Last Density :  0.4768472517484672\n",
      "++++ NEW Density :  0.5104349601010567\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03358770835258951\n",
      " Nb_Veh :  [10 31  5 30]\n",
      "\n",
      "++++ Phases density :  [0.255308947222479, 0.7521489971346705]\n",
      "++++ Last Density :  0.5104349601010567\n",
      "++++ NEW Density :  0.5037289721785747\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006705987922482004\n",
      " Nb_Veh :  [12 29  7 27]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.7387177650429799]\n",
      "++++ Last Density :  0.5037289721785747\n",
      "++++ NEW Density :  0.47014126382598515\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258956\n",
      " Nb_Veh :  [10 28  5 27]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224606, 0.8058739255014327]\n",
      "++++ Last Density :  0.47014126382598515\n",
      "++++ NEW Density :  0.49028329790183933\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020142034075854187\n",
      " Nb_Veh :  [ 9 30  4 30]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224606, 0.7790114613180515]\n",
      "++++ Last Density :  0.49028329790183933\n",
      "++++ NEW Density :  0.47685206581014883\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.013431232091690504\n",
      " Nb_Veh :  [ 9 30  4 28]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573462, 0.7387177650429799]\n",
      "++++ Last Density :  0.47685206581014883\n",
      "++++ NEW Density :  0.4835773099793573\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006725244169208444\n",
      " Nb_Veh :  [11 29  6 26]\n",
      "\n",
      "++++ Phases density :  [0.255308947222479, 0.7655802292263609]\n",
      "++++ Last Density :  0.4835773099793573\n",
      "++++ NEW Density :  0.51044458822442\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.026867278245062687\n",
      " Nb_Veh :  [12 29  7 28]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8058739255014327]\n",
      "++++ Last Density :  0.51044458822442\n",
      "++++ NEW Density :  0.4902784838401577\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02016610438426225\n",
      " Nb_Veh :  [ 8 30  5 30]\n",
      "\n",
      "++++ Phases density :  [0.18811427427057337, 0.8058739255014327]\n",
      "++++ Last Density :  0.4902784838401577\n",
      "++++ NEW Density :  0.496994099886003\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006715616045845307\n",
      " Nb_Veh :  [ 8 31  6 29]\n",
      "\n",
      "++++ Phases density :  [0.241858458884062, 0.7521489971346704]\n",
      "++++ Last Density :  0.496994099886003\n",
      "++++ NEW Density :  0.4970037280093662\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363192298e-06\n",
      " Nb_Veh :  [10 29  8 27]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.7521489971346704]\n",
      "++++ Last Density :  0.4970037280093662\n",
      "++++ NEW Density :  0.4835676818559941\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372128\n",
      " Nb_Veh :  [ 9 29  7 27]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.8193051575931232]\n",
      "++++ Last Density :  0.4835676818559941\n",
      "++++ NEW Density :  0.4902736697784761\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006705987922482004\n",
      " Nb_Veh :  [ 7 32  5 29]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.8193051575931233]\n",
      "++++ Last Density :  0.4902736697784761\n",
      "++++ NEW Density :  0.5104301460393752\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899114\n",
      " Nb_Veh :  [ 9 32  6 29]\n",
      "\n",
      "++++ Phases density :  [0.25529931909911574, 0.7252865329512894]\n",
      "++++ Last Density :  0.5104301460393752\n",
      "++++ NEW Density :  0.4902929260252026\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02013722001417262\n",
      " Nb_Veh :  [11 29  8 25]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7521489971346705]\n",
      "++++ Last Density :  0.4902929260252026\n",
      "++++ NEW Density :  0.49028811196352096\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [10 30  7 26]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.1746830421788828, 0.8058739255014327]\n",
      "++++ Last Density :  0.49028811196352096\n",
      "++++ NEW Density :  0.4902784838401577\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363247809e-06\n",
      " Nb_Veh :  [ 8 32  5 28]\n",
      "\n",
      "++++ Phases density :  [0.14781094987213853, 0.765580229226361]\n",
      "++++ Last Density :  0.4902784838401577\n",
      "++++ NEW Density :  0.45669558954924977\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.03358289429090794\n",
      " Nb_Veh :  [ 7 31  4 26]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7387177650429799]\n",
      "++++ Last Density :  0.45669558954924977\n",
      "++++ NEW Density :  0.4835724959176757\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026876906368425935\n",
      " Nb_Veh :  [10 30  7 25]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7790114613180517]\n",
      "++++ Last Density :  0.4835724959176757\n",
      "++++ NEW Density :  0.5037193440552116\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020146848137535867\n",
      " Nb_Veh :  [10 31  7 27]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.8327363896848138]\n",
      "++++ Last Density :  0.5037193440552116\n",
      "++++ NEW Density :  0.5037049018701667\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  1.4442185044871714e-05\n",
      " Nb_Veh :  [ 7 33  6 29]\n",
      "\n",
      "++++ Phases density :  [0.18811427427057337, 0.7924426934097422]\n",
      "++++ Last Density :  0.5037049018701667\n",
      "++++ NEW Density :  0.49027848384015776\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.013426418030008935\n",
      " Nb_Veh :  [ 8 32  6 27]\n",
      "\n",
      "++++ Phases density :  [0.26873055119080624, 0.73871776504298]\n",
      "++++ Last Density :  0.49027848384015776\n",
      "++++ NEW Density :  0.5037241581168932\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013445674276735431\n",
      " Nb_Veh :  [11 29  9 26]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.7118553008595988]\n",
      "++++ Last Density :  0.5037241581168932\n",
      "++++ NEW Density :  0.4634208337184582\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04030332439843498\n",
      " Nb_Veh :  [ 9 28  7 25]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.7521489971346705]\n",
      "++++ Last Density :  0.4634208337184582\n",
      "++++ NEW Density :  0.4566955895492498\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006725244169208389\n",
      " Nb_Veh :  [ 7 29  5 27]\n",
      "\n",
      "++++ Phases density :  [0.2284175986690082, 0.792442693409742]\n",
      "++++ Last Density :  0.4566955895492498\n",
      "++++ NEW Density :  0.5104301460393751\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.05373455649012526\n",
      " Nb_Veh :  [ 9 31  8 28]\n",
      "\n",
      "++++ Phases density :  [0.28216178328249686, 0.7118553008595989]\n",
      "++++ Last Density :  0.5104301460393751\n",
      "++++ NEW Density :  0.4970085420710479\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0134216039683272\n",
      " Nb_Veh :  [11 29 10 24]\n",
      "\n",
      "++++ Phases density :  [0.2284175986690082, 0.7118553008595988]\n",
      "++++ Last Density :  0.4970085420710479\n",
      "++++ NEW Density :  0.4701364497643035\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744366\n",
      " Nb_Veh :  [ 9 28  8 25]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.765580229226361]\n",
      "++++ Last Density :  0.4701364497643035\n",
      "++++ NEW Density :  0.49028329790183933\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02014684813753581\n",
      " Nb_Veh :  [ 9 29  7 28]\n",
      "\n",
      "++++ Phases density :  [0.17467341405551962, 0.8461676217765043]\n",
      "++++ Last Density :  0.49028329790183933\n",
      "++++ NEW Density :  0.510420517916012\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02013722001417262\n",
      " Nb_Veh :  [ 7 33  6 30]\n",
      "\n",
      "++++ Phases density :  [0.1478013217487753, 0.8058739255014327]\n",
      "++++ Last Density :  0.510420517916012\n",
      "++++ NEW Density :  0.47683762362510396\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.033582894290907994\n",
      " Nb_Veh :  [ 6 32  5 28]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.7655802292263612]\n",
      "++++ Last Density :  0.47683762362510396\n",
      "++++ NEW Density :  0.4835628677943125\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006725244169208555\n",
      " Nb_Veh :  [ 8 31  7 26]\n",
      "\n",
      "++++ Phases density :  [0.2015455063622639, 0.7790114613180517]\n",
      "++++ Last Density :  0.4835628677943125\n",
      "++++ NEW Density :  0.49027848384015776\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006715616045845252\n",
      " Nb_Veh :  [ 8 31  7 27]\n",
      "\n",
      "++++ Phases density :  [0.120929229442031, 0.8058739255014327]\n",
      "++++ Last Density :  0.49027848384015776\n",
      "++++ NEW Density :  0.46340157747173183\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026876906368425935\n",
      " Nb_Veh :  [ 5 32  4 28]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.8058739255014327]\n",
      "++++ Last Density :  0.46340157747173183\n",
      "++++ NEW Density :  0.48355805373263083\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899003\n",
      " Nb_Veh :  [ 7 32  5 28]\n",
      "\n",
      "++++ Phases density :  [0.241858458884062, 0.7790114613180515]\n",
      "++++ Last Density :  0.48355805373263083\n",
      "++++ NEW Density :  0.5104349601010567\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687690636842588\n",
      " Nb_Veh :  [10 30  8 28]\n",
      "\n",
      "++++ Phases density :  [0.18811427427057337, 0.7790114613180515]\n",
      "++++ Last Density :  0.5104349601010567\n",
      "++++ NEW Density :  0.48356286779431246\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744255\n",
      " Nb_Veh :  [ 8 30  6 28]\n",
      "\n",
      "++++ Phases density :  [0.13437971778044797, 0.8058739255014327]\n",
      "++++ Last Density :  0.48356286779431246\n",
      "++++ NEW Density :  0.47012682164094033\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372128\n",
      " Nb_Veh :  [ 7 30  3 30]\n",
      "\n",
      "++++ Phases density :  [0.14782057799550172, 0.8193051575931232]\n",
      "++++ Last Density :  0.47012682164094033\n",
      "++++ NEW Density :  0.48356286779431246\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013436046153372128\n",
      " Nb_Veh :  [ 8 31  3 30]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.7790114613180517]\n",
      "++++ Last Density :  0.48356286779431246\n",
      "++++ NEW Density :  0.490288111963521\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.006725244169208555\n",
      " Nb_Veh :  [10 31  5 27]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899034, 0.8058739255014327]\n",
      "++++ Last Density :  0.490288111963521\n",
      "++++ NEW Density :  0.5037193440552115\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690448\n",
      " Nb_Veh :  [10 31  5 29]\n",
      "\n",
      "++++ Phases density :  [0.14781094987213853, 0.8595988538681948]\n",
      "++++ Last Density :  0.5037193440552115\n",
      "++++ NEW Density :  0.5037049018701667\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  1.4442185044760691e-05\n",
      " Nb_Veh :  [ 7 33  4 31]\n",
      "\n",
      "++++ Phases density :  [0.13437008965708475, 0.8193051575931233]\n",
      "++++ Last Density :  0.5037049018701667\n",
      "++++ NEW Density :  0.476837623625104\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.026867278245062687\n",
      " Nb_Veh :  [ 6 32  4 29]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.7790114613180515]\n",
      "++++ Last Density :  0.476837623625104\n",
      "++++ NEW Density :  0.4969989139476846\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020161290322580572\n",
      " Nb_Veh :  [ 9 30  7 28]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.8058739255014327]\n",
      "++++ Last Density :  0.4969989139476846\n",
      "++++ NEW Density :  0.5171505761469021\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02015166219921749\n",
      " Nb_Veh :  [10 30  7 30]\n",
      "\n",
      "++++ Phases density :  [0.12093885756539419, 0.8193051575931232]\n",
      "++++ Last Density :  0.5171505761469021\n",
      "++++ NEW Density :  0.4701220075792587\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04702856856764337\n",
      " Nb_Veh :  [ 6 31  3 30]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8461676217765043]\n",
      "++++ Last Density :  0.4701220075792587\n",
      "++++ NEW Density :  0.5104253319776936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.04030332439843487\n",
      " Nb_Veh :  [ 8 32  5 31]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7924426934097422]\n",
      "++++ Last Density :  0.5104253319776936\n",
      "++++ NEW Density :  0.5104349601010568\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -9.628123363247809e-06\n",
      " Nb_Veh :  [10 30  7 29]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.7924426934097422]\n",
      "++++ Last Density :  0.5104349601010568\n",
      "++++ NEW Density :  0.48356286779431246\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744366\n",
      " Nb_Veh :  [ 8 30  5 29]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8461676217765044]\n",
      "++++ Last Density :  0.48356286779431246\n",
      "++++ NEW Density :  0.5104253319776936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02686246418338112\n",
      " Nb_Veh :  [ 8 32  5 31]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8327363896848137]\n",
      "++++ Last Density :  0.5104253319776936\n",
      "++++ NEW Density :  0.5037097159318482\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006715616045845363\n",
      " Nb_Veh :  [ 8 31  5 31]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.765580229226361]\n",
      "++++ Last Density :  0.5037097159318482\n",
      "++++ NEW Density :  0.4835676818559941\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.02014203407585413\n",
      " Nb_Veh :  [ 9 31  6 26]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.7790114613180515]\n",
      "++++ Last Density :  0.4835676818559941\n",
      "++++ NEW Density :  0.49028329790183933\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006715616045845252\n",
      " Nb_Veh :  [ 9 30  6 28]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8193051575931232]\n",
      "++++ Last Density :  0.49028329790183933\n",
      "++++ NEW Density :  0.49699409988600296\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006710801984163628\n",
      " Nb_Veh :  [ 8 31  5 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.1746830421788828, 0.8058739255014327]\n",
      "++++ Last Density :  0.49699409988600296\n",
      "++++ NEW Density :  0.4902784838401577\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006715616045845252\n",
      " Nb_Veh :  [ 8 31  5 29]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7521489971346704]\n",
      "++++ Last Density :  0.4902784838401577\n",
      "++++ NEW Density :  0.49028811196352096\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -9.628123363247809e-06\n",
      " Nb_Veh :  [10 28  7 28]\n",
      "\n",
      "++++ Phases density :  [0.20155513448562712, 0.765580229226361]\n",
      "++++ Last Density :  0.49028811196352096\n",
      "++++ NEW Density :  0.4835676818559941\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526876\n",
      " Nb_Veh :  [ 9 28  6 29]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.8058739255014327]\n",
      "++++ Last Density :  0.4835676818559941\n",
      "++++ NEW Density :  0.48356286779431246\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [ 8 30  4 30]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.8058739255014327]\n",
      "++++ Last Density :  0.48356286779431246\n",
      "++++ NEW Density :  0.5104349601010568\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026872092306744366\n",
      " Nb_Veh :  [10 30  6 30]\n",
      "\n",
      "++++ Phases density :  [0.2687401793141695, 0.765580229226361]\n",
      "++++ Last Density :  0.5104349601010568\n",
      "++++ NEW Density :  0.5171602042702652\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006725244169208389\n",
      " Nb_Veh :  [12 29  8 28]\n",
      "\n",
      "++++ Phases density :  [0.24186808700742518, 0.7790114613180517]\n",
      "++++ Last Density :  0.5171602042702652\n",
      "++++ NEW Density :  0.5104397741627384\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526765\n",
      " Nb_Veh :  [11 29  7 29]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.8327363896848137]\n",
      "++++ Last Density :  0.5104397741627384\n",
      "++++ NEW Density :  0.517150576146902\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006710801984163517\n",
      " Nb_Veh :  [10 31  5 31]\n",
      "\n",
      "++++ Phases density :  [0.24186808700742518, 0.8327363896848137]\n",
      "++++ Last Density :  0.517150576146902\n",
      "++++ NEW Density :  0.5373022383461195\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02015166219921749\n",
      " Nb_Veh :  [11 31  7 31]\n",
      "\n",
      "++++ Phases density :  [0.24186808700742518, 0.7521489971346704]\n",
      "++++ Last Density :  0.5373022383461195\n",
      "++++ NEW Density :  0.49700854207104783\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04029369627507162\n",
      " Nb_Veh :  [11 31  7 25]\n",
      "\n",
      "++++ Phases density :  [0.2687401793141695, 0.7790114613180515]\n",
      "++++ Last Density :  0.49700854207104783\n",
      "++++ NEW Density :  0.5238758203161105\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02686727824506263\n",
      " Nb_Veh :  [12 30  8 28]\n",
      "\n",
      "++++ Phases density :  [0.21499599470068087, 0.8193051575931233]\n",
      "++++ Last Density :  0.5238758203161105\n",
      "++++ NEW Density :  0.5171505761469021\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006725244169208389\n",
      " Nb_Veh :  [10 32  6 29]\n",
      "\n",
      "++++ Phases density :  [0.0940667652586499, 0.8595988538681948]\n",
      "++++ Last Density :  0.5171505761469021\n",
      "++++ NEW Density :  0.47683280956342233\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04031776658347974\n",
      " Nb_Veh :  [ 5 34  2 30]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8730300859598854]\n",
      "++++ Last Density :  0.47683280956342233\n",
      "++++ NEW Density :  0.5238565640693841\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.047023754505961746\n",
      " Nb_Veh :  [ 8 34  5 31]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7790114613180517]\n",
      "++++ Last Density :  0.5238565640693841\n",
      "++++ NEW Density :  0.5037193440552116\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.020137220014172508\n",
      " Nb_Veh :  [10 29  7 29]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.7790114613180517]\n",
      "++++ Last Density :  0.5037193440552116\n",
      "++++ NEW Density :  0.4768472517484672\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744366\n",
      " Nb_Veh :  [ 8 29  5 29]\n",
      "\n",
      "++++ Phases density :  [0.16125181008719228, 0.8327363896848137]\n",
      "++++ Last Density :  0.4768472517484672\n",
      "++++ NEW Density :  0.49699409988600296\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020146848137535756\n",
      " Nb_Veh :  [ 8 31  4 31]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.8327363896848137]\n",
      "++++ Last Density :  0.49699409988600296\n",
      "++++ NEW Density :  0.5238661921927473\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026872092306744366\n",
      " Nb_Veh :  [10 31  6 31]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.7924426934097422]\n",
      "++++ Last Density :  0.5238661921927473\n",
      "++++ NEW Density :  0.5037193440552116\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.020146848137535756\n",
      " Nb_Veh :  [10 31  6 28]\n",
      "\n",
      "++++ Phases density :  [0.2149959947006809, 0.8193051575931232]\n",
      "++++ Last Density :  0.5037193440552116\n",
      "++++ NEW Density :  0.5171505761469021\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013431232091690504\n",
      " Nb_Veh :  [10 31  6 30]\n",
      "\n",
      "++++ Phases density :  [0.1881335305172998, 0.8595988538681949]\n",
      "++++ Last Density :  0.5171505761469021\n",
      "++++ NEW Density :  0.5238661921927473\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006715616045845252\n",
      " Nb_Veh :  [10 32  4 32]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224603, 0.8461676217765043]\n",
      "++++ Last Density :  0.5238661921927473\n",
      "++++ NEW Density :  0.5104301460393752\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.013436046153372128\n",
      " Nb_Veh :  [ 9 32  4 31]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573465, 0.8058739255014327]\n",
      "++++ Last Density :  0.5104301460393752\n",
      "++++ NEW Density :  0.5171553902085837\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.0067252441692085\n",
      " Nb_Veh :  [11 31  6 29]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573462, 0.8058739255014327]\n",
      "++++ Last Density :  0.5171553902085837\n",
      "++++ NEW Density :  0.5171553902085837\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0\n",
      " Nb_Veh :  [11 31  6 29]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8327363896848138]\n",
      "++++ Last Density :  0.5171553902085837\n",
      "++++ NEW Density :  0.5037097159318483\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013445674276735375\n",
      " Nb_Veh :  [ 8 32  5 30]\n",
      "\n",
      "++++ Phases density :  [0.21498636657731768, 0.8327363896848138]\n",
      "++++ Last Density :  0.5037097159318483\n",
      "++++ NEW Density :  0.5238613781310657\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02015166219921738\n",
      " Nb_Veh :  [ 9 32  7 30]\n",
      "\n",
      "++++ Phases density :  [0.26873055119080624, 0.8058739255014327]\n",
      "++++ Last Density :  0.5238613781310657\n",
      "++++ NEW Density :  0.5373022383461195\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013440860215053752\n",
      " Nb_Veh :  [11 30  9 30]\n",
      "\n",
      "++++ Phases density :  [0.241858458884062, 0.8058739255014327]\n",
      "++++ Last Density :  0.5373022383461195\n",
      "++++ NEW Density :  0.5238661921927473\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372128\n",
      " Nb_Veh :  [10 29  8 31]\n",
      "\n",
      "++++ Phases density :  [0.1746830421788828, 0.8461676217765044]\n",
      "++++ Last Density :  0.5238661921927473\n",
      "++++ NEW Density :  0.5104253319776936\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013440860215053752\n",
      " Nb_Veh :  [ 8 31  5 32]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.8327363896848138]\n",
      "++++ Last Density :  0.5104253319776936\n",
      "++++ NEW Density :  0.5305818082385926\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899003\n",
      " Nb_Veh :  [10 30  7 32]\n",
      "\n",
      "++++ Phases density :  [0.22842722679237143, 0.7924426934097422]\n",
      "++++ Last Density :  0.5305818082385926\n",
      "++++ NEW Density :  0.5104349601010568\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.020146848137535756\n",
      " Nb_Veh :  [10 30  7 29]\n",
      "\n",
      "++++ Phases density :  [0.25529931909911574, 0.7924426934097422]\n",
      "++++ Last Density :  0.5104349601010568\n",
      "++++ NEW Density :  0.523871006254429\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013436046153372128\n",
      " Nb_Veh :  [11 30  8 29]\n",
      "\n",
      "++++ Phases density :  [0.20156476260899037, 0.8327363896848137]\n",
      "++++ Last Density :  0.523871006254429\n",
      "++++ NEW Density :  0.517150576146902\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006720430107526987\n",
      " Nb_Veh :  [10 31  5 31]\n",
      "\n",
      "++++ Phases density :  [0.1881335305172998, 0.8327363896848137]\n",
      "++++ Last Density :  0.517150576146902\n",
      "++++ NEW Density :  0.5104349601010567\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.006715616045845252\n",
      " Nb_Veh :  [10 31  4 31]\n",
      "\n",
      "++++ Phases density :  [0.26874980743753274, 0.8193051575931232]\n",
      "++++ Last Density :  0.5104349601010567\n",
      "++++ NEW Density :  0.544027482515328\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03359252241427124\n",
      " Nb_Veh :  [13 32  7 29]\n",
      "\n",
      "++++ Phases density :  [0.26874980743753274, 0.8327363896848138]\n",
      "++++ Last Density :  0.544027482515328\n",
      "++++ NEW Density :  0.5507430985611732\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006715616045845252\n",
      " Nb_Veh :  [13 32  7 30]\n",
      "\n",
      "++++ Phases density :  [0.17469267030224603, 0.8730300859598854]\n",
      "++++ Last Density :  0.5507430985611732\n",
      "++++ NEW Density :  0.5238613781310657\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026881720430107503\n",
      " Nb_Veh :  [ 9 33  4 32]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573465, 0.8595988538681948]\n",
      "++++ Last Density :  0.5238613781310657\n",
      "++++ NEW Density :  0.5440178543919647\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899003\n",
      " Nb_Veh :  [11 33  6 31]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.3090531318359676, 0.8327363896848138]\n",
      "++++ Last Density :  0.5440178543919647\n",
      "++++ NEW Density :  0.5708947607603907\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.02687690636842599\n",
      " Nb_Veh :  [14 31  9 31]\n",
      "\n",
      "++++ Phases density :  [0.255308947222479, 0.8193051575931232]\n",
      "++++ Last Density :  0.5708947607603907\n",
      "++++ NEW Density :  0.5373070524078011\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258962\n",
      " Nb_Veh :  [12 30  7 31]\n",
      "\n",
      "++++ Phases density :  [0.21499599470068087, 0.8595988538681949]\n",
      "++++ Last Density :  0.5373070524078011\n",
      "++++ NEW Density :  0.5372974242844379\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  9.628123363136787e-06\n",
      " Nb_Veh :  [10 31  6 33]\n",
      "\n",
      "++++ Phases density :  [0.2821810395292233, 0.8461676217765044]\n",
      "++++ Last Density :  0.5372974242844379\n",
      "++++ NEW Density :  0.5641743306528638\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02687690636842588\n",
      " Nb_Veh :  [13 31  8 32]\n",
      "\n",
      "++++ Phases density :  [0.2821810395292233, 0.7655802292263612]\n",
      "++++ Last Density :  0.5641743306528638\n",
      "++++ NEW Density :  0.5238806343777922\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.04029369627507162\n",
      " Nb_Veh :  [13 26  8 31]\n",
      "\n",
      "++++ Phases density :  [0.2821810395292233, 0.765580229226361]\n",
      "++++ Last Density :  0.5238806343777922\n",
      "++++ NEW Density :  0.5238806343777922\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0\n",
      " Nb_Veh :  [13 27  8 30]\n",
      "\n",
      "++++ Phases density :  [0.20157439073235356, 0.8058739255014327]\n",
      "++++ Last Density :  0.5238806343777922\n",
      "++++ NEW Density :  0.5037241581168931\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.020156476260899114\n",
      " Nb_Veh :  [11 28  4 32]\n",
      "\n",
      "++++ Phases density :  [0.22844648303909787, 0.8193051575931232]\n",
      "++++ Last Density :  0.5037241581168931\n",
      "++++ NEW Density :  0.5238758203161105\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.02015166219921738\n",
      " Nb_Veh :  [12 30  5 31]\n",
      "\n",
      "++++ Phases density :  [0.2821906676525865, 0.8058739255014327]\n",
      "++++ Last Density :  0.5238758203161105\n",
      "++++ NEW Density :  0.5440322965770096\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020156476260899114\n",
      " Nb_Veh :  [14 30  7 30]\n",
      "\n",
      "++++ Phases density :  [0.2821906676525865, 0.8058739255014327]\n",
      "++++ Last Density :  0.5440322965770096\n",
      "++++ NEW Density :  0.5440322965770096\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0\n",
      " Nb_Veh :  [14 30  7 30]\n",
      "\n",
      "++++ Phases density :  [0.20157439073235356, 0.8595988538681949]\n",
      "++++ Last Density :  0.5440322965770096\n",
      "++++ NEW Density :  0.5305866223002742\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013445674276735375\n",
      " Nb_Veh :  [11 32  4 32]\n",
      "\n",
      "++++ Phases density :  [0.25531857534584224, 0.8461676217765042]\n",
      "++++ Last Density :  0.5305866223002742\n",
      "++++ NEW Density :  0.5507430985611732\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.020156476260899003\n",
      " Nb_Veh :  [13 32  6 31]\n",
      "\n",
      "++++ Phases density :  [0.3090627599593308, 0.7924426934097422]\n",
      "++++ Last Density :  0.5507430985611732\n",
      "++++ NEW Density :  0.5507527266845365\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -9.628123363247809e-06\n",
      " Nb_Veh :  [15 30  8 29]\n",
      "\n",
      "++++ Phases density :  [0.25531857534584224, 0.7790114613180517]\n",
      "++++ Last Density :  0.5507527266845365\n",
      "++++ NEW Density :  0.517165018331947\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03358770835258951\n",
      " Nb_Veh :  [13 30  6 28]\n",
      "\n",
      "++++ Phases density :  [0.20158401885571678, 0.8058739255014327]\n",
      "++++ Last Density :  0.517165018331947\n",
      "++++ NEW Density :  0.5037289721785747\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.013436046153372239\n",
      " Nb_Veh :  [12 31  3 29]\n",
      "\n",
      "++++ Phases density :  [0.25532820346920543, 0.8193051575931232]\n",
      "++++ Last Density :  0.5037289721785747\n",
      "++++ NEW Density :  0.5373166805311643\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.03358770835258962\n",
      " Nb_Veh :  [14 31  5 30]\n",
      "\n",
      "++++ Phases density :  [0.25532820346920543, 0.8058739255014327]\n",
      "++++ Last Density :  0.5373166805311643\n",
      "++++ NEW Density :  0.5306010644853191\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006715616045845252\n",
      " Nb_Veh :  [14 31  5 29]\n",
      "\n",
      "++++ Phases density :  [0.2822002957759497, 0.8058739255014327]\n",
      "++++ Last Density :  0.5306010644853191\n",
      "++++ NEW Density :  0.5440371106386912\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013436046153372128\n",
      " Nb_Veh :  [15 31  6 29]\n",
      "\n",
      "++++ Phases density :  [0.25532820346920543, 0.8327363896848138]\n",
      "++++ Last Density :  0.5440371106386912\n",
      "++++ NEW Density :  0.5440322965770096\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  4.8140616816239046e-06\n",
      " Nb_Veh :  [14 32  5 30]\n",
      "\n",
      "++++ Phases density :  [0.20157439073235356, 0.9267550143266475]\n",
      "++++ Last Density :  0.5440322965770096\n",
      "++++ NEW Density :  0.5641647025295006\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.020132405952490995\n",
      " Nb_Veh :  [11 35  4 34]\n",
      "\n",
      "++++ Phases density :  [0.21501525094740734, 0.913323782234957]\n",
      "++++ Last Density :  0.5641647025295006\n",
      "++++ NEW Density :  0.5641695165911822\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -4.8140616816239046e-06\n",
      " Nb_Veh :  [12 34  4 34]\n",
      "\n",
      "++++ Phases density :  [0.29563152786764024, 0.8058739255014327]\n",
      "++++ Last Density :  0.5641695165911822\n",
      "++++ NEW Density :  0.5507527266845365\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  0.013416789906645743\n",
      " Nb_Veh :  [15 30  7 30]\n",
      "\n",
      "++++ Phases density :  [0.24188734325415168, 0.8058739255014327]\n",
      "++++ Last Density :  0.5507527266845365\n",
      "++++ NEW Density :  0.5238806343777922\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.026872092306744255\n",
      " Nb_Veh :  [13 30  5 30]\n",
      "\n",
      "++++ Phases density :  [0.17471192654897247, 0.8595988538681949]\n",
      "++++ Last Density :  0.5238806343777922\n",
      "++++ NEW Density :  0.5171553902085837\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0067252441692085\n",
      " Nb_Veh :  [11 32  2 32]\n",
      "\n",
      "++++ Phases density :  [0.25532820346920543, 0.8595988538681949]\n",
      "++++ Last Density :  0.5171553902085837\n",
      "++++ NEW Density :  0.5574635286687002\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.040308138460116494\n",
      " Nb_Veh :  [14 32  5 32]\n",
      "\n",
      "++++ Phases density :  [0.29563152786764024, 0.8058739255014327]\n",
      "++++ Last Density :  0.5574635286687002\n",
      "++++ NEW Density :  0.5507527266845365\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006710801984163739\n",
      " Nb_Veh :  [15 31  7 29]\n",
      "\n",
      "++++ Phases density :  [0.25532820346920543, 0.7924426934097422]\n",
      "++++ Last Density :  0.5507527266845365\n",
      "++++ NEW Density :  0.5238854484394738\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02686727824506263\n",
      " Nb_Veh :  [14 30  5 29]\n",
      "\n",
      "++++ Phases density :  [0.20158401885571678, 0.8327363896848137]\n",
      "++++ Last Density :  0.5238854484394738\n",
      "++++ NEW Density :  0.5171602042702652\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006725244169208611\n",
      " Nb_Veh :  [12 31  3 31]\n",
      "\n",
      "++++ Phases density :  [0.2284561111624611, 0.8327363896848137]\n",
      "++++ Last Density :  0.5171602042702652\n",
      "++++ NEW Density :  0.5305962504236373\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.013436046153372128\n",
      " Nb_Veh :  [13 31  4 31]\n",
      "\n",
      "++++ Phases density :  [0.28220029577594974, 0.7924426934097422]\n",
      "++++ Last Density :  0.5305962504236373\n",
      "++++ NEW Density :  0.537321494592846\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006725244169208611\n",
      " Nb_Veh :  [15 29  6 30]\n",
      "\n",
      "++++ Phases density :  [0.28220029577594974, 0.7790114613180517]\n",
      "++++ Last Density :  0.537321494592846\n",
      "++++ NEW Density :  0.5306058785470007\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.006715616045845252\n",
      " Nb_Veh :  [15 29  6 29]\n",
      "\n",
      "++++ Phases density :  [0.24189697137751487, 0.8327363896848137]\n",
      "++++ Last Density :  0.5306058785470007\n",
      "++++ NEW Density :  0.5373166805311642\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006710801984163517\n",
      " Nb_Veh :  [14 31  4 31]\n",
      "\n",
      "++++ Phases density :  [0.17470229842560925, 0.9133237822349569]\n",
      "++++ Last Density :  0.5373166805311642\n",
      "++++ NEW Density :  0.5440130403302831\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.006696359799118867\n",
      " Nb_Veh :  [10 34  3 34]\n",
      "\n",
      "++++ Phases density :  [0.22844648303909787, 0.913323782234957]\n",
      "++++ Last Density :  0.5440130403302831\n",
      "++++ NEW Density :  0.5708851326370274\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026872092306744366\n",
      " Nb_Veh :  [12 34  5 34]\n",
      "\n",
      "++++ Phases density :  [0.2821906676525865, 0.7924426934097422]\n",
      "++++ Last Density :  0.5708851326370274\n",
      "++++ NEW Density :  0.5373166805311643\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.03356845210586312\n",
      " Nb_Veh :  [14 29  7 30]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++ Phases density :  [0.25531857534584224, 0.7790114613180517]\n",
      "++++ Last Density :  0.5373166805311643\n",
      "++++ NEW Density :  0.517165018331947\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.02015166219921738\n",
      " Nb_Veh :  [13 29  6 29]\n",
      "\n",
      "++++ Phases density :  [0.22843685491573465, 0.8327363896848137]\n",
      "++++ Last Density :  0.517165018331947\n",
      "++++ NEW Density :  0.5305866223002742\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  -0.013421603968327256\n",
      " Nb_Veh :  [11 31  6 31]\n",
      "\n",
      "++++ Phases density :  [0.16124218196382906, 0.8864613180515759]\n",
      "++++ Last Density :  0.5305866223002742\n",
      "++++ NEW Density :  0.5238517500077025\n",
      "+++++++ INFO +++++++++\n",
      " Action :  0\n",
      " Reward :  0.0067348722925717475\n",
      " Nb_Veh :  [ 7 33  5 33]\n",
      "\n",
      "++++ Phases density :  [0.18811427427057337, 0.913323782234957]\n",
      "++++ Last Density :  0.5238517500077025\n",
      "++++ NEW Density :  0.5507190282527652\n",
      "+++++++ INFO +++++++++\n",
      " Action :  1\n",
      " Reward :  -0.026867278245062742\n",
      " Nb_Veh :  [ 8 34  6 34]\n",
      "\n"
     ]
    },
    {
     "ename": "FatalTraCIError",
     "evalue": "Connection closed by SUMO.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m      6\u001b[0m     action, _state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m     obs, rewards, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m3\u001b[39m\u001b[38;5;66;03m#obs, rewards, dones, info = env.step(action)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     done \u001b[38;5;241m=\u001b[39m truncated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:228\u001b[0m, in \u001b[0;36mSumoEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# if self.fixed_ts:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m#     self.traffic_signal.sumo.trafficlight.setRedYellowGreenState(\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m#             self.ts_id, self.traffic_signal.all_phases[self.fixed_ts_phase_id].state\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_action(action)\n\u001b[1;32m--> 228\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_steps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_observation()\n\u001b[0;32m    231\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_reward()\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:243\u001b[0m, in \u001b[0;36mSumoEnvironment._run_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    241\u001b[0m time_to_act \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m time_to_act:\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sumo_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_signal\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraffic_signal\u001b[38;5;241m.\u001b[39mtime_to_act:\n",
      "File \u001b[1;32mE:\\Instigo-TS-CTRL\\CustomGymEnvSetup\\environment\\env.py:313\u001b[0m, in \u001b[0;36mSumoEnvironment._sumo_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sumo_step\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msumo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimulationStep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:368\u001b[0m, in \u001b[0;36mConnection.simulationStep\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(step) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mint\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m:\n\u001b[0;32m    367\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPI change now handles step as floating point seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 368\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendCmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCMD_SIMSTEP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subscriptionResults \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subscriptionMapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m    370\u001b[0m     subscriptionResults\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:231\u001b[0m, in \u001b[0;36mConnection._sendCmd\u001b[1;34m(self, cmdID, varID, objID, format, *values)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39mpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(objID)) \u001b[38;5;241m+\u001b[39m objID\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_string \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m packed\n\u001b[1;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendExact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\connection.py:137\u001b[0m, in \u001b[0;36mConnection._sendExact\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_socket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FatalTraCIError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection closed by SUMO.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m command \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue:\n\u001b[0;32m    139\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!BBB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: Connection closed by SUMO."
     ]
    }
   ],
   "source": [
    "episode = 1\n",
    "#obs, info = env.reset()\n",
    "for i in range(episode):\n",
    "    done=False\n",
    "    while not done:\n",
    "        action, _state = model.predict(obs, deterministic=True)\n",
    "        obs, rewards, terminated, truncated, info = env.step(action)\n",
    "        3#obs, rewards, dones, info = env.step(action)\n",
    "        done = truncated\n",
    "\n",
    "        print(f\"+++++++ INFO +++++++++\")\n",
    "        print(f\" Action : \", action)\n",
    "        print(f\" Reward : \", rewards)\n",
    "        print(f\" Nb_Veh : \", obs['nb_veh'])\n",
    "        print(f\"\")\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4070f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d11f6e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
